<div align="center">
  <img src="./image/title.png" width="800" />
</div>
<div align="center">
<strong>ğŸŒâœ¨Summarizing the latest LLMs and VLMs! Helping you quickly and easily choose and use large models! ğŸ˜„</strong><br>
<strong>This is the repository navigation page, the main Awesome List: <a href="./README_LLM.md">LLMsğŸš€</a></strong> | <strong><a href="./README_VLMs.md">VLMsğŸš€</a></strong><br>
<strong>Supported languages: <a href="./README_zh.md">ä¸­æ–‡ğŸš€</a></strong> | <strong>English</strong>
</div>
<br>

Welcome to our repositoryğŸ¥°, a comprehensive navigation page that connects you to the most relevant resources and summary platforms for the latest large models(including <a href="./README_LLM.md">LLMsğŸš€</a> and <a href="./README_VLMs.md">VLMsğŸš€</a>). Whether you're looking for benchmarksğŸ’¯, comparisonsâš–ï¸, or surveysğŸ“–, we've got you covered. **this navigation page also links to other relevant summary platforms**. Explore the sections below to find the information you need:

- **Benchmarking Inference Speed of Large Language Models**ğŸš€

[GPU-Benchmarks-on-LLM-Inference](https://github.com/XiongjieDai/GPU-Benchmarks-on-LLM-Inference) uses various NVIDIA GPUs and Apple Silicon devices to test models like LLaMA 3 with the llama.cpp tool, measuring performance by tokens generated per second. It covers NVIDIA 3000, 4000, and A100 series, as well as Apple's M1, M2, and M3 chips.

- **Comprehensive Analysis and Comparison of Large Language Models**ğŸ”

The website [LifeArchitect.ai/models](https://lifearchitect.ai/models) provides a comprehensive analysis and comparison of large language models (LLMs) such as GPT-3, GPT-4, and PaLM, detailing their sizes, capabilities, and training data.

- **Reliable Measurement of Large Language Model Response Times**â±ï¸

[TheFastest.ai](https://thefastest.ai/) offers reliable performance measurements for popular large language models (LLMs) based on response times. It compares models across multiple data centers (e.g., US West, East, and Europe), focusing on metrics like Time to First Token (TTFT) and Tokens Per Second (TPS), with daily updated statistics.

- **Comprehensive Survey of Vision-Language Models**ğŸ“Š

[VLM_survey](https://github.com/jingyi0000/VLM_survey) is a repository summarizing and surveying the latest vision-language models (VLMs), including links to relevant papers. It covers:

1. **Overview of Vision-Language Models**: Reviews VLM research in image classification, object detection, and semantic segmentation.
2. **Pre-training Methods**: Summarizes network architectures, pre-training objectives, and downstream tasks for VLMs.
3. **Transfer Learning Methods**: Discusses transfer learning strategies for VLMs in different tasks.
4. **Knowledge Distillation Methods**: Examines knowledge distillation techniques in tasks like object detection and semantic segmentation.

