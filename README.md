# awesome-list-of-LLM_VLMsğŸŒâœ¨
github awesome list of recent LLMs and VLMs.

Here is list of **LLMs**, to reach list of VLMs. Click [here](https://github.com/HuBocheng/awesome-list-of-LLM_VLMs/blob/master/README_VLMs.md)ğŸš€



## Quick StartğŸ

|  Model  | Organization |       Parameters        |                          CheckPoint                          |      Details       |
| :-----: | :----------: | :---------------------: | :----------------------------------------------------------: | :----------------: |
| Llama 2 |     Meta     |       7B/13B/70B        | [Llama2 Family](https://huggingface.co/collections/meta-llama/llama-2-family-661da1f90a9d678b6f55773b) | [Llama2](#llama2)  |
| Llama 3 |     Meta     |         8B/70B          | [Llama3 Family](https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6) | [Llama3](#llama3)  |
|  Qwen   |   Alibaba    | 0.5B/1.8B/4B/7B/14B/72B |             [Qwen](https://huggingface.co/Qwen)              |   [Qwen](#qwen)    |
| Qwen1.5 |   Alibaba    | 0.5B/1.8B/4B/7B/14B/72B |            [Qwen1.5](https://huggingface.co/Qwen)            | [Qwen1.5](#qwen15) |
| Vicuna  |    LMSYS     |       7B/13B/33B        |            [Vicuna](https://huggingface.co/lmsys)            | [Vicuna](#vicuna)  |
|  XGen   |  Salesforce  |           7B            | [xgen-7b-4k-base](https://huggingface.co/Salesforce/xgen-7b-4k-base) |   [XGen](#xgen)    |
| Falcon  |     UAE      |   1.3B/7.5B/40B/180B    | [Falcon Family](https://huggingface.co/collections/tiiuae/falcon-64fb432660017eeec9837b5a) | [Falcon](#falcon)  |
|   phi   |  Microsoft   |       1B/1.5B/2B        | [phi-1B()](https://huggingface.co/microsoft/phi-1)<br />[phi-1.5B](https://huggingface.co/microsoft/phi-1_5)<br />[phi-2B](https://huggingface.co/microsoft/phi-2) |    [phi](#phi)     |
|  phi3   |  Microsoft   |       3.8B/7B/14B       | [Phi-3 family **(only phi-3-mini is available now)**](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3) |   [phi3](#phi3)    |
|  Gemma  |    Google    |          2B/7B          | [GemmaFamily](https://huggingface.co/collections/google/gemma-release-65d5efbccdbb8c4202ec078b) |  [Gemma](#gemma)   |



## Details Regarding Models AboveğŸ“Š

### Llama2

[![arXiv](https://img.shields.io/badge/arXiv-2307.09288-b31b1b.svg)](https://arxiv.org/abs/2307.09288) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/meta-llama/llama)
[![Hugging Face model](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-models-blue)](https://huggingface.co/collections/meta-llama/llama-2-family-661da1f90a9d678b6f55773b)


Llama 2é¢„è®­ç»ƒæ¨¡å‹ç›¸è¾ƒäº Llama 1 æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼Œå¢åŠ äº† 40% çš„è®­ç»ƒè¯å…ƒæ€»æ•°ï¼Œå¹¶é‡‡ç”¨äº†æ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆé«˜è¾¾ 4000 è¯å…ƒï¼‰ï¼ŒåŒæ—¶è¿˜åˆ©ç”¨åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶ï¼Œæå¤§åŠ é€Ÿäº† 70B æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€‚Llama 2-Chat ç³»åˆ—æ¨¡å‹é‡‡ç”¨äº†åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æŠ€æœ¯ï¼Œä¸“é—¨é’ˆå¯¹å¯¹è¯åœºæ™¯è¿›è¡Œä¼˜åŒ–ã€‚åœ¨å¹¿æ³›çš„æœ‰ç”¨æ€§å’Œå®‰å…¨æ€§æµ‹è¯•åŸºå‡†ä¸­ï¼ŒLlama 2-Chat çš„è¡¨ç°è¶…è¿‡äº†å¤šæ•°ç°æœ‰å¼€æ”¾æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨äººç±»è¯„ä¼°ä¸­æ˜¾ç¤ºå‡ºä¸ ChatGPT ç›¸åª²ç¾çš„æ€§èƒ½ã€‚

- **Date:** 2023-07
- **Pretrain Data Scale:** 2T
- **Language Support:** en
- **Parameter Size:** 7B/13B/70B



### Llama3

[![AI Blog](https://img.shields.io/badge/AI%20Blog-Meta%20AI-orange.svg)](https://ai.meta.com/blog/)
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/meta-llama/llama3)
[![Hugging Face model](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-models-blue)](https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6)

ç›¸è¾ƒäº Llama 2ï¼ŒLlama 3 æœ€æ˜¾è‘—çš„å˜åŒ–åœ¨äºå¼•å…¥äº†ä¸€ä¸ªæ–°çš„ Tokenizerï¼Œå¹¶å°†è¯æ±‡è¡¨çš„è§„æ¨¡æ‰©å±•åˆ°äº† 128,256 ä¸ªè¯æ±‡ï¼ˆå…ˆå‰ç‰ˆæœ¬ä¸º 32,000 ä¸ª Tokenï¼‰ã€‚è¿™æ ·ä¸€ä¸ªæ›´åºå¤§çš„è¯æ±‡è¡¨å¯ä»¥æ›´æœ‰æ•ˆåœ°å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œæ— è®ºæ˜¯è¾“å…¥è¿˜æ˜¯è¾“å‡ºï¼Œä¹Ÿå¯èƒ½å¢å¼ºæ¨¡å‹å¤„ç†å¤šè¯­è¨€çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ä¸€æ”¹å˜åŒæ—¶å¯¼è‡´äº†åµŒå…¥å±‚çš„è¾“å…¥å’Œè¾“å‡ºçŸ©é˜µçš„å°ºå¯¸å¢å¤§ï¼Œä»è€Œå¢åŠ äº†å°å‹æ¨¡å‹çš„å‚æ•°é‡ã€‚

- **Date:** 2024-04
- **Pretrain Data Scale:** 1.5T
- **Language Support:** en
- **Parameter Size:** 8B/70B



### Qwen

[![arXiv](https://img.shields.io/badge/arXiv-2309.16609-b31b1b.svg)](https://arxiv.org/abs/2309.16609) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/QwenLM/Qwen) 
[![Hugging Face model](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-models-blue)](https://huggingface.co/Qwen)

é€šè¿‡æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Fine-Tuningï¼‰å¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œä¼˜åŒ–ï¼Œæ¯”å¦‚ç¼–ç¨‹è¯­è¨€ç”Ÿæˆï¼ˆCodeQwenï¼‰ï¼Œé‡‡ç”¨äº†æ··åˆä¸“å®¶ï¼ˆMixture of Experts, MoEï¼‰å’Œç¨€ç–æ¿€æ´»æŠ€æœ¯.

- **Date:** 2023-08
- **Pretrain Data Scale:** 2.2T~3T(1.8B:2.2T;7B:2.4T;14B:3.0T)
- **Language Support:** en,zh
- **Parameter Size:** 0.5B/1.8B/4B/7B/14B/72B



### Qwen1.5

[![AI Blog](https://img.shields.io/badge/AI%20Blog-QWen%20AI-orange.svg)](https://qwen.readthedocs.io/en/latest/) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/QwenLM/Qwen1.5) 
[![Hugging Face model](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-models-blue)](https://huggingface.co/Qwen)

Qwen1.5çš„å„ä¸ªæ¨¡å‹åœ¨è®­ç»ƒä¸Šä½¿ç”¨äº†æ›´å¤šçš„Tokenï¼Œå¹¶é’ˆå¯¹ç‰¹å®šä»»åŠ¡å¦‚ä»£ç ç”Ÿæˆè¿›è¡Œäº†æŒ‡ä»¤å¾®è°ƒã€‚ä¾‹å¦‚ï¼ŒCodeQwen1.5ä¸“é—¨é’ˆå¯¹ç¼–ç¨‹ä»»åŠ¡è¿›è¡Œäº†ä¼˜åŒ–ï¼Œé¢„è®­ç»ƒäº†å¤§çº¦3ä¸‡äº¿ä¸ªä¸ä»£ç ç›¸å…³çš„æ•°æ®Tokenã€‚

- **Date:** 2023-02
- **Pretrain Data Scale:** 3T
- **Language Support:** en,zh
- **Parameter Size:** 0.5B/1.8B/4B/7B/14B/72B



### Vicuna

[![AI Blog](https://img.shields.io/badge/AI%20Blog-Vicuna%20AI-orange.svg)](https://lmsys.org/blog/2023-03-30-vicuna/) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/lm-sys/FastChat) 
[![Hugging Face model](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-models-blue)](https://huggingface.co/lmsys)

1. **å†…å­˜ä¼˜åŒ–**ï¼šä¸ºåº”å¯¹Vicunaåœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶çš„éœ€æ±‚ï¼Œå…¶æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ä»Alpacaçš„512å¢è‡³2048ï¼Œè¿™æ˜¾è‘—æé«˜äº†å¯¹GPUå†…å­˜çš„éœ€æ±‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶äººå‘˜é‡‡ç”¨äº†æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆgradient checkpointingï¼‰ä¸FlashAttentionæŠ€æœ¯æ¥å‡è½»å†…å­˜å‹åŠ›ã€‚
2. **å¤šè½®å¯¹è¯**ï¼šé€šè¿‡è°ƒæ•´è®­ç»ƒæŸå¤±ä»¥é€‚åº”å¤šè½®å¯¹è¯çš„éœ€æ±‚ï¼Œè®­ç»ƒæŸå¤±çš„è®¡ç®—ä»…åŸºäºèŠå¤©æœºå™¨äººçš„è¾“å‡ºè¿›è¡Œã€‚
3. **é€šè¿‡Spotå®ä¾‹é™ä½æˆæœ¬**ï¼šæ•°æ®é›†è§„æ¨¡å¢å¤§40å€åŠåºåˆ—é•¿åº¦å¢åŠ 4å€ï¼Œå¯¹è®­ç»ƒæå‡ºäº†æ›´å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºé™ä½æˆæœ¬ï¼Œç ”ç©¶äººå‘˜é€šè¿‡SkyPilotæ‰˜ç®¡çš„Spotå®ä¾‹ï¼Œåˆ©ç”¨æŠ¢å è‡ªåŠ¨æ¢å¤å’Œè‡ªåŠ¨åŒºåŸŸåˆ‡æ¢åŠŸèƒ½ï¼Œä½¿ç”¨æˆæœ¬æ›´ä½çš„Spotå®ä¾‹ã€‚è¿™ç§ç­–ç•¥å°†7Bæ¨¡å‹çš„è®­ç»ƒæˆæœ¬ä»500ç¾å…ƒé™è‡³çº¦140ç¾å…ƒï¼Œ13Bæ¨¡å‹çš„è®­ç»ƒæˆæœ¬ä»å¤§çº¦1000ç¾å…ƒé™è‡³300ç¾å…ƒã€‚

- **Date:** 2023-03
- **Pretrain Data Scale:** 1.4T
- **Language Support:** en,zh
- **Parameter Size:** 7B/13B/33B



### XGen

[![arXiv](https://img.shields.io/badge/arXiv-2309.03450-b31b1b.svg)](https://arxiv.org/abs/2309.03450) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/salesforce/xGen) 
[![Hugging Face model](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-models-blue)](https://huggingface.co/Salesforce/xgen-7b-4k-base)

XGen-7Bæ¨¡å‹åœ¨æ”¯æŒé•¿è¾¾8Kä»¤ç‰Œçš„è¾“å…¥ï¼Œé€šè¿‡ä½¿ç”¨æ ‡å‡†å¯†é›†æ³¨æ„åŠ›è¿›è¡Œè®­ç»ƒï¼Œä»¥åŠåœ¨é«˜è¾¾1.5Tä»¤ç‰Œçš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶åœ¨å…¬å…±é¢†åŸŸçš„æ•™å­¦æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¯ä½œä¸ºä¸€ç§é€šç”¨æ¨¡å‹ï¼Œé€‚ç”¨äºæ ‡å‡†å¤§å°çš„GPUå’Œç§»åŠ¨è®¾å¤‡ã€‚

- **Date:** 2023-07
- **Pretrain Data Scale:** 1.37T
- **Language Support:** en
- **Parameter Size:** 7B



### Falcon

[![AI Blog](https://img.shields.io/badge/AI%20Blog-Falcon%20AI-orange.svg)](https://huggingface.co/blog/falcon-180b) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/falconry/falcon) 
[![Hugging Face model](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-models-blue)](https://huggingface.co/tiiuae)

å¤§é‡è®­ç»ƒæ•°æ®æ¥è‡ª [RefinedWeb](https://arxiv.org/abs/2306.01116) â€”â€” ä¸€ä¸ªæ–°çš„åŸºäº CommonCrawl çš„ç½‘ç»œæ•°æ®é›†ã€‚ä½¿ç”¨äº† [**å¤šæŸ¥è¯¢æ³¨æ„åŠ› (multiquery attention)**](https://arxiv.org/abs/1911.02150)ã€‚åŸå§‹å¤šå¤´ (head) æ³¨æ„åŠ›æ–¹æ¡ˆæ¯ä¸ªå¤´éƒ½åˆ†åˆ«æœ‰ä¸€ä¸ªæŸ¥è¯¢ (query) ã€é”® (key) ä»¥åŠå€¼ (value)ï¼Œè€Œå¤šæŸ¥è¯¢æ³¨æ„åŠ›æ–¹æ¡ˆæ”¹ä¸ºåœ¨æ‰€æœ‰å¤´ä¸Šå…±äº«åŒä¸€ä¸ªé”®å’Œå€¼ã€‚è¿™ä¸ªæŠ€å·§å¯¹é¢„è®­ç»ƒå½±å“ä¸å¤§ï¼Œä½†å®ƒæå¤§åœ° [æé«˜äº†æ¨ç†çš„å¯æ‰©å±•æ€§](https://arxiv.org/abs/2211.05102): äº‹å®ä¸Šï¼Œ **è¯¥æŠ€å·§å¤§å¤§å‡å°‘äº†è‡ªå›å½’è§£ç æœŸé—´ K,V ç¼“å­˜çš„å†…å­˜å ç”¨ï¼Œå°†å…¶å‡å°‘äº† 10-100 å€** (å…·ä½“æ•°å€¼å–å†³äºæ¨¡å‹æ¶æ„çš„é…ç½®)ï¼Œè¿™å¤§å¤§é™ä½äº†æ¨¡å‹æ¨ç†çš„å†…å­˜å¼€é”€ã€‚è€Œå†…å­˜å¼€é”€çš„å‡å°‘ä¸ºè§£é”æ–°çš„ä¼˜åŒ–å¸¦æ¥äº†å¯èƒ½ï¼Œå¦‚çœä¸‹æ¥çš„å†…å­˜å¯ä»¥ç”¨æ¥å­˜å‚¨å†å²å¯¹è¯ï¼Œä»è€Œä½¿å¾—æœ‰çŠ¶æ€æ¨ç†æˆä¸ºå¯èƒ½ã€‚

- **Date:** 2023-07
- **Pretrain Data Scale:** 2T
- **Language Support:** en,fr
- **Parameter Size:** 1.3B/7.5B/40B/180B



### Phi

[![AI Blog](https://img.shields.io/badge/AI%20Blog-Phi%20AI-orange.svg)](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/microsoft/dstoolkit-phi2-finetune) 
[![Hugging Face model](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-models-blue)](https://huggingface.co/microsoft/phi-2)

**è®­ç»ƒæ•°æ®è´¨é‡ï¼š** åˆ©ç”¨â€œæ•™ç§‘ä¹¦è´¨é‡â€çš„æ•°æ®ï¼Œä¸“æ³¨äºæ—¨åœ¨ä¼ æˆå¸¸è¯†æ¨ç†å’Œå¸¸è¯†çš„åˆæˆæ•°æ®é›†ã€‚åŸ¹è®­è¯­æ–™åº“é€šè¿‡ç²¾å¿ƒæŒ‘é€‰çš„ç½‘ç»œæ•°æ®è¿›è¡Œæ‰©å……ï¼Œå¹¶æ ¹æ®æ•™è‚²ä»·å€¼å’Œå†…å®¹è´¨é‡è¿›è¡Œè¿‡æ»¤ã€‚ï¼ˆPhi-2 leverages â€œtextbook-qualityâ€ data, focusing on synthetic datasets designed to impart common sense reasoning and general knowledge. The training corpus is augmented with carefully selected web data, filtered based on educational value and content quality.ï¼‰

**åˆ›æ–°çš„ç¼©æ”¾æŠ€æœ¯ï¼š** å¾®è½¯åœ¨å¼€å‘ Phi-2 æ—¶ï¼Œé‡‡ç”¨äº†ä»å…¶å‰ä»£æ¨¡å‹ Phi-1.5 åˆ° Phi-2 çš„çŸ¥è¯†æ‰©å±•æŠ€æœ¯ã€‚ä»–ä»¬åˆ©ç”¨äº† Phi-1.5 æ¨¡å‹ä¸­å·²æœ‰çš„çŸ¥è¯†å’Œå­¦ä¹ æˆæœï¼Œå°†è¿™äº›çŸ¥è¯†è½¬ç§»åˆ°æ–°æ¨¡å‹ä¸­ï¼Œä»è€ŒåŠ é€Ÿäº†æ–°æ¨¡å‹è®­ç»ƒçš„æ”¶æ•›é€Ÿåº¦ã€‚ç®€å•æ¥è¯´ï¼Œå°±æ˜¯è®©æ–°æ¨¡å‹åœ¨å­¦ä¹ åˆæœŸå°±èƒ½ç«™åœ¨ä¸€ä¸ªæ›´é«˜çš„èµ·ç‚¹ä¸Šï¼Œå¿«é€Ÿè¾¾åˆ°é«˜æ€§èƒ½ã€‚è¿™ç§çŸ¥è¯†è½¬ç§»çš„æ–¹æ³•ä¸ä»…æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œè¿˜æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­çš„å¾—åˆ†ã€‚

**ä¼˜åŒ–çš„Transformerç»“æ„**ï¼šPhi-2ç ”ç©¶äººå‘˜å¼•å…¥äº†è‡ªå®šä¹‰ä¼˜åŒ–ä»¥æœ€å¤§é™åº¦åœ°æé«˜æ•ˆç‡ã€‚

- **Date:** 2023-12
- **Pretrain Data Scale:** 1.4T
- **Language Support:** en
- **Parameter Size:** 1B/1.5B/2B



### Phi3

[![arXiv](https://img.shields.io/badge/arXiv-2404.14219-b31b1b.svg)](https://arxiv.org/abs/2404.14219) 
[![Hugging Face model](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-models-blue)](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)

é‡‡ç”¨äº†transformer decoderæ¶æ„ï¼Œå…¶é»˜è®¤çš„ä¸Šä¸‹æ–‡é•¿åº¦ä¸º**4K token**ã€‚å›¢é˜Ÿè¿˜é€šè¿‡LongRopeæŠ€æœ¯æ¨å‡ºäº†é•¿ä¸Šä¸‹æ–‡ç‰ˆæœ¬ï¼Œå°†ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•åˆ°**128K token**ã€‚ç”±äºä½“ç§¯å°å·§ï¼Œphi-3-miniæ¨¡å‹å¯ä»¥è¢«é‡åŒ–ä¸º**4 bit**ï¼Œä½¿å…¶å†…å­˜å ç”¨ä»…çº¦ä¸º**1.8GB**ã€‚å›¢é˜Ÿé€šè¿‡åœ¨é…å¤‡A16ä»¿ç”ŸèŠ¯ç‰‡çš„iPhone 14ä¸Šéƒ¨ç½²phi-3-miniè¿›è¡Œäº†æµ‹è¯•ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å®Œå…¨ç¦»çº¿çš„çŠ¶æ€ä¸‹åŸç”Ÿè¿è¡Œï¼Œå¹¶å®ç°äº†æ¯ç§’å¤„ç†è¶…è¿‡12ä¸ªtokensçš„é€Ÿåº¦ã€‚

è®­ç»ƒæ•°æ®é›†ä¹Ÿæ˜¯åˆ›æ–°ç‚¹ä¹‹ä¸€ï¼ŒåŒ…æ‹¬ç»è¿‡ä¸¥æ ¼è¿‡æ»¤çš„ç½‘ç»œæ•°æ®å’Œåˆæˆæ•°æ®ï¼Œè¿™äº›æ•°æ®ç»è¿‡ç²¾å¿ƒç­›é€‰å’Œä¼˜åŒ–ï¼Œä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿæ˜¾è‘—å‡å°æ¨¡å‹çš„å¤§å°è€Œä¸å½±å“æ€§èƒ½ã€‚

- **Date:** 2024-04
- **Pretrain Data Scale:** 3.3Tï½4.8T
- **Language Support:** en
- **Parameter Size:** 3.8B/7B/14B



### Gemma

[![AI Blog](https://img.shields.io/badge/AI%20Blog-Phi%20AI-orange.svg)](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/google-deepmind/gemma) 
[![Hugging Face model](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-models-blue)](https://huggingface.co/collections/google/gemma-release-65d5efbccdbb8c4202ec078b)



å¯åœ¨å„ç±»æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šè¿è¡Œï¼Œæ— éœ€æ•°æ®é‡åŒ–å¤„ç†ï¼Œæ‹¥æœ‰é«˜è¾¾ 8K tokens çš„å¤„ç†èƒ½åŠ›ï¼Œåœ¨ 7B å‚æ•°çº§åˆ«Gemma è¡¨ç°å‡ºè‰²ï¼Œæ¯”åŒå‚æ•°çº§åˆ«çš„Llama2æ€§èƒ½è¦å¥½ä¸€äº›ã€‚ä¸ Google Cloud é›†æˆï¼Œå¯ä»¥é€šè¿‡ Vertex AI æˆ– Google Kubernetes Engine (GKE) åœ¨ Google Cloud ä¸Šéƒ¨ç½²å’Œè®­ç»ƒ Gemmaã€‚

- **Date:** 2024-02
- **Pretrain Data Scale:** 2T
- **Language Support:** en
- **Parameter Size:** 2B/7B



## æ¨¡å‹è¯„ä¼°å¹³å°

### OpenLLM Leaderboard by Hugging Face[[Leaderboard]](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)

Hugging Faceçš„OpenLLMæ’è¡Œæ¦œæ˜¯ä¸€ä¸ªæ—¨åœ¨è¿½è¸ªã€è¯„çº§å’Œè¯„ä¼°å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’ŒèŠå¤©æœºå™¨äººçš„å¹³å°ã€‚è¿™ä¸ªæ’è¡Œæ¦œæä¾›äº†ä¸€ä¸ªé›†ä¸­çš„å¹³å°ã€‚

[Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) is actually just a wrapper running the open-source benchmarking library [Eleuther AI LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) created by the [EleutherAI non-profit AI research lab](https://www.eleuther.ai/) famous for creating [The Pile](https://pile.eleuther.ai/) and training [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6b), [GPT-Neo-X 20B](https://huggingface.co/EleutherAI/gpt-neox-20b), and [Pythia](https://github.com/EleutherAI/pythia). A team with serious credentials in the AI space!ï¼ˆCitation from Hugging Face blogï¼‰



### Chatbot Arena by LMSYS and SkyLab[[Chatbot Arena]](https://arena.lmsys.org/)

Chatbot Arenaçš„ä¸»è¦åŠŸèƒ½ä»‹ç»ï¼š

1. **æ¨¡å‹å¯¹æˆ˜ï¼ˆArena Battleï¼‰**ï¼šåœ¨Chatbot Arenaä¸­ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä¸¤ä¸ªä¸åŒçš„åŒ¿åæ¨¡å‹ï¼ˆå¦‚ChatGPTã€Claudeã€Llamaç­‰ï¼‰ï¼Œå¹¶å°†å®ƒä»¬è¿›è¡Œå¯¹æ¯”ã€‚åœ¨ä¸€ä¸ªå®‰å…¨çš„æ¯”èµ›ç¯å¢ƒä¸­ï¼Œç”¨æˆ·å¯ä»¥æé—®å¹¶è¯„ä¼°ä¸¤ä¸ªæ¨¡å‹çš„å›ç­”ï¼Œé€šè¿‡æŠ•ç¥¨å†³å®šå“ªä¸ªæ¨¡å‹è¡¨ç°æ›´å‡ºè‰²ã€‚è¿™ä¸€æ¯”è¾ƒè¿‡ç¨‹å¯ä»¥å¤šè½®è¿›è¡Œï¼Œç›´åˆ°ç¡®å®šæœ€ç»ˆçš„èƒœåˆ©è€…ã€‚ä¸ºäº†ä¿è¯æ¯”èµ›çš„å…¬å¹³æ€§ï¼Œå¦‚æœæŸè½®å¯¹è¯æš´éœ²äº†æ¨¡å‹çš„å…·ä½“èº«ä»½ï¼Œè¯¥è½®çš„æŠ•ç¥¨ç»“æœå°†è¢«æ’é™¤ã€‚
2. **å®æ—¶èŠå¤©ï¼ˆDirect Chatï¼‰**ï¼šChatbot Arenaæä¾›äº†ä¸€ä¸ªå®æ—¶èŠå¤©åŠŸèƒ½ï¼Œå…è®¸ç”¨æˆ·ç›´æ¥ä¸é€‰å®šçš„æ¨¡å‹è¿›è¡Œå¯¹è¯ã€‚æ— è®ºç”¨æˆ·é€‰æ‹©çš„æ˜¯æ–‡æœ¬è¿˜æ˜¯è§†è§‰èŠå¤©æ¨¡å¼ï¼Œéƒ½èƒ½å³æ—¶æ¥æ”¶åˆ°æ¨¡å‹çš„åé¦ˆã€‚
3. **æ’è¡Œæ¦œï¼ˆLeaderboardï¼‰**ï¼šChatbot Arenaé€šè¿‡åˆ†æè¶…è¿‡300,000ä¸ªäººç±»ç”¨æˆ·çš„æŠ•ç¥¨ç»“æœï¼Œé‡‡ç”¨åŸºäºEloç³»ç»Ÿçš„æ–¹æ³•æ¥è¯„ä¼°å„ä¸ªLLMçš„æ’åã€‚è¿™è®©ç”¨æˆ·èƒ½å¤Ÿç›´è§‚åœ°äº†è§£å“ªäº›æ¨¡å‹å½“å‰å¤„äºé¢†å…ˆåœ°ä½ï¼Œè½»æ¾è¯†åˆ«å‡ºLLMé¢†åŸŸçš„ä½¼ä½¼è€…ã€‚



## Benchmarks

### ARC (AI2 Reasoning Challenge)

[![Paper with Code](https://img.shields.io/badge/Paper%20with%20Code-arc%20challenge-blue.svg)](https://paperswithcode.com/sota/common-sense-reasoning-on-arc-challenge)
[![Paper with Code](https://img.shields.io/badge/Paper%20with%20Code-arc%20easy-blue.svg)](https://paperswithcode.com/sota/common-sense-reasoning-on-arc-easy)
[![arXiv](https://img.shields.io/badge/arXiv-2305.18354-b31b1b.svg)](https://arxiv.org/pdf/2305.18354) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/fchollet/ARC) 

The ARC (Abstraction and Reasoning Corpus) benchmark is a challenging framework used to evaluate the performance of large language models, particularly in their ability to perform tasks that require both abstraction and reasoning. This benchmark is part of an effort to push the boundaries of what AI models can achieve beyond standard pattern recognition tasks that are common in many benchmarks today.

##### Key Features of ARC

- **Task Design**: The tasks in ARC are designed to be straightforward for humans but challenging for AI. Each task involves an input and a correct output, with the goal being to predict the output from the input by discerning the underlying pattern or rule. The tasks cover various types of cognitive functions, including pattern recognition, spatial reasoning, and logical deduction.
- **Dataset**: The ARC dataset is publicly available and consists of a training set and a test set, where the test set contains completely novel tasks that require generalizing beyond the training data. This setup is intended to simulate real-world learning scenarios where direct answers are not always available, and reasoning must be applied.
- **Evaluation**: Success in ARC requires developing models that can truly understand and manipulate abstract concepts, making it a stringent test of an AIâ€™s reasoning abilities rather than just its data processing capabilities.



### HellaSwag

[![Paper with Code](https://img.shields.io/badge/Paper%20with%20Code-hellaswag-blue.svg)](https://paperswithcode.com/sota/sentence-completion-on-hellaswag)
[![arXiv](https://img.shields.io/badge/arXiv-1905.07830-b31b1b.svg)](https://arxiv.org/abs/1905.07830) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/rowanz/hellaswag) 

**HellaSwag** is a benchmark designed to evaluate the performance of language models on commonsense natural language inference. It specifically tests the model's ability to predict the continuation of straightforward descriptions of everyday activities, which are obvious to humans but potentially challenging for algorithms. The benchmark consists of numerous multiple-choice questions, each providing a scenario and four possible continuations, from which the model must select the most appropriate one.

The questions in HellaSwag primarily derive from two domains: ActivityNet and WikiHow, enriching the dataset with practical relevance and diversity. The correct answer describes the actual next event, while the other three incorrect options are adversarially generated and human-verified to mislead models but not humans.

The construction of HellaSwag includes an "Adversarial Filtering" technique, where a series of discriminators iteratively select machine-generated wrong answers that effectively deceive models. This method has proven its effectiveness in challenging models to recognize real-world scenarios and has highlighted the limitations of even state-of-the-art models.



### MMLU (Massive Multitask Language Understanding)

[![Paper with Code](https://img.shields.io/badge/Paper%20with%20Code-MMLU-blue.svg)](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu?tag_filter=183)
[![arXiv](https://img.shields.io/badge/arXiv-2009.03300-b31b1b.svg)](https://arxiv.org/abs/2009.03300) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/hendrycks/test) 

This is a broad and diverse benchmark that evaluates a model's understanding across a wide range of subjects, from humanities to science. It's designed to test the model's general knowledge and understanding capabilities on various topics.

> æ³¨æ„ï¼šMMLUçš„è¯„ä¼°åŒ…å«ä¸‰ç§å½¢å¼ï¼Œåˆ†åˆ«æ˜¯MMLU (HELM)ã€MMLU (Harness)å’ŒMMLU (Original)ã€‚æ›¾ç»ï¼Œåœ¨Hugging Faceçš„OpenLLMæ’è¡Œæ¦œä¸Šï¼Œ[**LLaMA model ğŸ¦™**](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)è¢«çˆ†å‡ºMMLUè¯„ä¼°ç»“æœæ˜æ˜¾ä½äº[å·²å‘è¡¨çš„ LLaMa è®ºæ–‡](https://arxiv.org/abs/2302.13971)ä¸­çš„æ•°å­—ã€‚æœ€ç»ˆ[Blog](https://huggingface.co/blog/open-llm-leaderboard-mmlu)ä¸Šçš„å£°æ˜å°†è¿™ä¸ªç°è±¡å½’ç»“ä¸ºç”¨äºæµ‹è¯•çš„MMLU benchmarkçš„ç»†èŠ‚ä¸åŒå¯¼è‡´äº†ç»“æœä¸åŒï¼Œè¯¦æƒ…å¯å‚è§[Blog](https://huggingface.co/blog/open-llm-leaderboard-mmlu)



### TruthfulQA

[![Paper with Code](https://img.shields.io/badge/Paper%20with%20Code-truthfulqa-blue.svg)](https://paperswithcode.com/sota/question-answering-on-truthfulqa)
[![arXiv](https://img.shields.io/badge/arXiv-2109.07958-b31b1b.svg)](https://arxiv.org/abs/2109.07958) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/sylinrl/TruthfulQA) 

This benchmark evaluates a model's ability to provide truthful answers. It focuses on minimizing the spread of misinformation by testing how models handle questions that might typically lead to untruthful or misleading answers.

**TruthfulQA** is a benchmark designed to assess the truthfulness of answers generated by large language models. It consists of 817 questions across 38 categories, including sectors like health, law, finance, and politics. The questions are designed to probe models on their ability to handle scenarios where humans might respond incorrectly due to misconceptions or false beliefs. The main goal of TruthfulQA is to measure how accurately models can generate truthful responses. The benchmark employs various tasks and modes, including single and multiple-choice questions as well as generative tasks, to evaluate a model's capacity to identify and generate truthful answers.

##### Tasks and Evaluation Methods

TruthfulQA includes two primary tasks:

1. **Generation Task**: Models are given a question and must generate a one to two-sentence answer. The primary metric for this task is the overall truthfulness of the modelâ€™s answers, assessed against a standard where the truth must describe the literal truth about the real world. Secondary metrics include informativeness, assessed by whether the model provides meaningful information rather than evasive responses.
2. **Multiple-Choice Task**: This task comes in two forms:
   - **MC1 (Single-true)**: Models are provided with a question and several answer choices, and they must select the single correct answer.
   - **MC2 (Multi-true)**: Models are given multiple true and false reference answers and must identify all correct answers.

##### Important Points

- **Adversarial Nature**: The questions in TruthfulQA are adversarially designed to test models on potential weaknesses in generating truthful answers. This includes crafting questions that leverage common misconceptions or that models might commonly answer falsely due to their training on biased data distributions.
- **Validation and Benchmarks**: The benchmark includes validation mechanisms where external researchers assess the truthfulness and accuracy of the reference answers, ensuring the robustness of the benchmark's evaluations.



### Winogrande

[![Paper with Code](https://img.shields.io/badge/Paper%20with%20Code-Winogrande-blue.svg)](https://paperswithcode.com/sota/common-sense-reasoning-on-winogrande)
[![arXiv](https://img.shields.io/badge/arXiv-1907.10641-b31b1b.svg)](https://arxiv.org/pdf/1907.10641) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/allenai/winogrande) 

WINOGRANDE is a language understanding benchmark based on the Winograd Schema Challenge (WSC) designed to evaluate the performance of large language models (LLMs) in solving pronoun resolution problems that require common sense and reasoning abilities. By offering a larger and more challenging dataset than the original WSC, WINOGRANDE is better suited for assessing the capabilities of modern artificial intelligence language models.

Developed by the Allen Institute for AI, the purpose of the WINOGRANDE benchmark is to test models' reasoning abilities through pronoun resolution tasks. These tasks typically involve a sentence with a pronoun and multiple potential referents, and the model needs to determine the correct referent for the pronoun. For example:

```less
Sentence: The elephant put the apple there because it was ______.
Options: A) empty B) heavy
Correct Answer: A) empty
```

This type of question requires the model to understand not just the literal meaning of the language but also to reason about implicit relationships within the context.



### GSM8K (Grade School Math 8K)

[![Paper with Code](https://img.shields.io/badge/Paper%20with%20Code-GSM8K-blue.svg)](https://paperswithcode.com/dataset/gsm8k)
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/openai/grade-school-math) 

The GSM8K (Grade School Math 8K) dataset, developed by OpenAI, is designed to evaluate the performance of large language models on solving elementary school-level math problems. The benchmark consists of 8,500 high-quality, linguistically diverse math word problems that require basic arithmetic operations such as addition, subtraction, multiplication, and division. These problems typically take between two to eight steps to solve. The dataset is divided into 7,500 training problems and 1,000 test problems, aimed at testing the models' capabilities in multi-step mathematical reasoning. GSM8K is particularly useful for assessing how well models handle problems that involve sequential logic and computational steps.



## Quantitative metrics for large model inference

Quantitative metrics for large model inference using NVIDIA GPUs and Apple Silicon chips, please reach [here](https://github.com/XiongjieDai/GPU-Benchmarks-on-LLM-Inference)

