# awesome-list-of-LLM_VLMsğŸŒâœ¨
github awesome list of recent LLMs and VLMs.

Here is list of LLMs, to reach list of VLMs. Click [here](https://github.com/HuBocheng/awesome-list-of-LLM_VLMs/blob/master/README_VLMs.md)ğŸš€

## Quick StartğŸ

|  Model  | Organization |       Parameters        |                          CheckPoint                          | Details |
| :-----: | :----------: | :---------------------: | :----------------------------------------------------------: | ------- |
| Llama 2 |     Meta     |       7B/13B/70B        | [Llama2 Family](https://huggingface.co/collections/meta-llama/llama-2-family-661da1f90a9d678b6f55773b) |         |
| Llama 3 |     Meta     |         8B/70B          | [Llama3 Family](https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6) |         |
|  Qwen   |   Alibaba    | 0.5B/1.8B/4B/7B/14B/72B |             [Qwen](https://huggingface.co/Qwen)              |         |
| Qwen1.5 |   Alibaba    | 0.5B/1.8B/4B/7B/14B/72B |            [Qwen1.5](https://huggingface.co/Qwen)            |         |
| Vicuna  |    LMSYS     |       7B/13B/33B        |            [Vicuna](https://huggingface.co/lmsys)            |         |
|  XGen   |  Salesforce  |           7B            | [xgen-7b-4k-base](https://huggingface.co/Salesforce/xgen-7b-4k-base) |         |
| Falcon  |     UAE      |   1.3B/7.5B/40B/180B    | [Falcon Family](https://huggingface.co/collections/tiiuae/falcon-64fb432660017eeec9837b5a) |         |
|   phi   |  Microsoft   |       1B/1.5B/2B        | [phi-1B()](https://huggingface.co/microsoft/phi-1)<br />[phi-1.5B](https://huggingface.co/microsoft/phi-1_5)<br />[phi-2B](https://huggingface.co/microsoft/phi-2) |         |
|  phi3   |  Microsoft   |       3.8B/7B/14B       | [Phi-3 family **(only phi-3-mini is available now)**](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3) |         |
|  Gemma  |    Google    |          2B/7B          | [GemmaFamily](https://huggingface.co/collections/google/gemma-release-65d5efbccdbb8c4202ec078b) |         |


[123](#phi3)
[234](#Llama2)
## Details Regarding Models AboveğŸ“Š

### Llama 2ğŸ¦™

[![arXiv](https://img.shields.io/badge/arXiv-2307.09288-b31b1b.svg)](https://arxiv.org/abs/2307.09288) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/meta-llama/llama)
[![Hugging Face Space](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/collections/meta-llama/llama-2-family-661da1f90a9d678b6f55773b)


Llama 2é¢„è®­ç»ƒæ¨¡å‹ç›¸è¾ƒäº Llama 1 æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼Œå¢åŠ äº† 40% çš„è®­ç»ƒè¯å…ƒæ€»æ•°ï¼Œå¹¶é‡‡ç”¨äº†æ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆé«˜è¾¾ 4000 è¯å…ƒï¼‰ï¼ŒåŒæ—¶è¿˜åˆ©ç”¨åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶ï¼Œæå¤§åŠ é€Ÿäº† 70B æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€‚Llama 2-Chat ç³»åˆ—æ¨¡å‹é‡‡ç”¨äº†åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æŠ€æœ¯ï¼Œä¸“é—¨é’ˆå¯¹å¯¹è¯åœºæ™¯è¿›è¡Œä¼˜åŒ–ã€‚åœ¨å¹¿æ³›çš„æœ‰ç”¨æ€§å’Œå®‰å…¨æ€§æµ‹è¯•åŸºå‡†ä¸­ï¼ŒLlama 2-Chat çš„è¡¨ç°è¶…è¿‡äº†å¤šæ•°ç°æœ‰å¼€æ”¾æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨äººç±»è¯„ä¼°ä¸­æ˜¾ç¤ºå‡ºä¸ ChatGPT ç›¸åª²ç¾çš„æ€§èƒ½ã€‚

- **Date:** 2023-07
- **Pretrain Data Scale:** 2T
- **Language Support:** en
- **Parameter Size:** 7B/13B/70B



### Llama 3ğŸ¦™

[![AI Blog](https://img.shields.io/badge/AI%20Blog-Meta%20AI-orange.svg)](https://ai.meta.com/blog/)
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/meta-llama/llama3)
[![Hugging Face Space](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6)

ç›¸è¾ƒäº Llama 2ï¼ŒLlama 3 æœ€æ˜¾è‘—çš„å˜åŒ–åœ¨äºå¼•å…¥äº†ä¸€ä¸ªæ–°çš„ Tokenizerï¼Œå¹¶å°†è¯æ±‡è¡¨çš„è§„æ¨¡æ‰©å±•åˆ°äº† 128,256 ä¸ªè¯æ±‡ï¼ˆå…ˆå‰ç‰ˆæœ¬ä¸º 32,000 ä¸ª Tokenï¼‰ã€‚è¿™æ ·ä¸€ä¸ªæ›´åºå¤§çš„è¯æ±‡è¡¨å¯ä»¥æ›´æœ‰æ•ˆåœ°å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œæ— è®ºæ˜¯è¾“å…¥è¿˜æ˜¯è¾“å‡ºï¼Œä¹Ÿå¯èƒ½å¢å¼ºæ¨¡å‹å¤„ç†å¤šè¯­è¨€çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ä¸€æ”¹å˜åŒæ—¶å¯¼è‡´äº†åµŒå…¥å±‚çš„è¾“å…¥å’Œè¾“å‡ºçŸ©é˜µçš„å°ºå¯¸å¢å¤§ï¼Œä»è€Œå¢åŠ äº†å°å‹æ¨¡å‹çš„å‚æ•°é‡ã€‚

- **Date:** 2024-04
- **Pretrain Data Scale:** 1.5T
- **Language Support:** en
- **Parameter Size:** 8B/70B



### Qwen

[![arXiv](https://img.shields.io/badge/arXiv-2309.16609-b31b1b.svg)](https://arxiv.org/abs/2309.16609) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/QwenLM/Qwen) 
[![Hugging Face Space](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/Qwen)

é€šè¿‡æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Fine-Tuningï¼‰å¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œä¼˜åŒ–ï¼Œæ¯”å¦‚ç¼–ç¨‹è¯­è¨€ç”Ÿæˆï¼ˆCodeQwenï¼‰ï¼Œé‡‡ç”¨äº†æ··åˆä¸“å®¶ï¼ˆMixture of Experts, MoEï¼‰å’Œç¨€ç–æ¿€æ´»æŠ€æœ¯.

- **Date:** 2023-08
- **Pretrain Data Scale:** 2.2T~3T(1.8B:2.2T;7B:2.4T;14B:3.0T)
- **Language Support:** en,zh
- **Parameter Size:** 0.5B/1.8B/4B/7B/14B/72B



### Qwen1.5

[![AI Blog](https://img.shields.io/badge/AI%20Blog-QWen%20AI-orange.svg)](https://qwen.readthedocs.io/en/latest/) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/QwenLM/Qwen1.5) 
[![Hugging Face Space](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/Qwen)

Qwen1.5çš„å„ä¸ªæ¨¡å‹åœ¨è®­ç»ƒä¸Šä½¿ç”¨äº†æ›´å¤šçš„Tokenï¼Œå¹¶é’ˆå¯¹ç‰¹å®šä»»åŠ¡å¦‚ä»£ç ç”Ÿæˆè¿›è¡Œäº†æŒ‡ä»¤å¾®è°ƒã€‚ä¾‹å¦‚ï¼ŒCodeQwen1.5ä¸“é—¨é’ˆå¯¹ç¼–ç¨‹ä»»åŠ¡è¿›è¡Œäº†ä¼˜åŒ–ï¼Œé¢„è®­ç»ƒäº†å¤§çº¦3ä¸‡äº¿ä¸ªä¸ä»£ç ç›¸å…³çš„æ•°æ®Tokenã€‚

- **Date:** 2023-02
- **Pretrain Data Scale:** 3T
- **Language Support:** en,zh
- **Parameter Size:** 0.5B/1.8B/4B/7B/14B/72B



### Vicuna

[![AI Blog](https://img.shields.io/badge/AI%20Blog-Vicuna%20AI-orange.svg)](https://lmsys.org/blog/2023-03-30-vicuna/) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/lm-sys/FastChat) 
[![Hugging Face Space](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/lmsys)

1. **å†…å­˜ä¼˜åŒ–**ï¼šä¸ºåº”å¯¹Vicunaåœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶çš„éœ€æ±‚ï¼Œå…¶æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ä»Alpacaçš„512å¢è‡³2048ï¼Œè¿™æ˜¾è‘—æé«˜äº†å¯¹GPUå†…å­˜çš„éœ€æ±‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶äººå‘˜é‡‡ç”¨äº†æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆgradient checkpointingï¼‰ä¸FlashAttentionæŠ€æœ¯æ¥å‡è½»å†…å­˜å‹åŠ›ã€‚
2. **å¤šè½®å¯¹è¯**ï¼šé€šè¿‡è°ƒæ•´è®­ç»ƒæŸå¤±ä»¥é€‚åº”å¤šè½®å¯¹è¯çš„éœ€æ±‚ï¼Œè®­ç»ƒæŸå¤±çš„è®¡ç®—ä»…åŸºäºèŠå¤©æœºå™¨äººçš„è¾“å‡ºè¿›è¡Œã€‚
3. **é€šè¿‡Spotå®ä¾‹é™ä½æˆæœ¬**ï¼šæ•°æ®é›†è§„æ¨¡å¢å¤§40å€åŠåºåˆ—é•¿åº¦å¢åŠ 4å€ï¼Œå¯¹è®­ç»ƒæå‡ºäº†æ›´å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºé™ä½æˆæœ¬ï¼Œç ”ç©¶äººå‘˜é€šè¿‡SkyPilotæ‰˜ç®¡çš„Spotå®ä¾‹ï¼Œåˆ©ç”¨æŠ¢å è‡ªåŠ¨æ¢å¤å’Œè‡ªåŠ¨åŒºåŸŸåˆ‡æ¢åŠŸèƒ½ï¼Œä½¿ç”¨æˆæœ¬æ›´ä½çš„Spotå®ä¾‹ã€‚è¿™ç§ç­–ç•¥å°†7Bæ¨¡å‹çš„è®­ç»ƒæˆæœ¬ä»500ç¾å…ƒé™è‡³çº¦140ç¾å…ƒï¼Œ13Bæ¨¡å‹çš„è®­ç»ƒæˆæœ¬ä»å¤§çº¦1000ç¾å…ƒé™è‡³300ç¾å…ƒã€‚

- **Date:** 2023-03
- **Pretrain Data Scale:** 1.4T
- **Language Support:** en,zh
- **Parameter Size:** 7B/13B/33B



### XGen

[![arXiv](https://img.shields.io/badge/arXiv-2309.03450-b31b1b.svg)](https://arxiv.org/abs/2309.03450) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/salesforce/xGen) 
[![Hugging Face Space](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/Salesforce/xgen-7b-4k-base)

XGen-7Bæ¨¡å‹åœ¨æ”¯æŒé•¿è¾¾8Kä»¤ç‰Œçš„è¾“å…¥ï¼Œé€šè¿‡ä½¿ç”¨æ ‡å‡†å¯†é›†æ³¨æ„åŠ›è¿›è¡Œè®­ç»ƒï¼Œä»¥åŠåœ¨é«˜è¾¾1.5Tä»¤ç‰Œçš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶åœ¨å…¬å…±é¢†åŸŸçš„æ•™å­¦æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¯ä½œä¸ºä¸€ç§é€šç”¨æ¨¡å‹ï¼Œé€‚ç”¨äºæ ‡å‡†å¤§å°çš„GPUå’Œç§»åŠ¨è®¾å¤‡ã€‚

- **Date:** 2023-07
- **Pretrain Data Scale:** 1.37T
- **Language Support:** en
- **Parameter Size:** 7B



### Falcon

[![AI Blog](https://img.shields.io/badge/AI%20Blog-Falcon%20AI-orange.svg)](https://huggingface.co/blog/falcon-180b) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/falconry/falcon) 
[![Hugging Face Space](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/tiiuae)

å¤§é‡è®­ç»ƒæ•°æ®æ¥è‡ª [RefinedWeb](https://arxiv.org/abs/2306.01116) â€”â€” ä¸€ä¸ªæ–°çš„åŸºäº CommonCrawl çš„ç½‘ç»œæ•°æ®é›†ã€‚ä½¿ç”¨äº† [**å¤šæŸ¥è¯¢æ³¨æ„åŠ› (multiquery attention)**](https://arxiv.org/abs/1911.02150)ã€‚åŸå§‹å¤šå¤´ (head) æ³¨æ„åŠ›æ–¹æ¡ˆæ¯ä¸ªå¤´éƒ½åˆ†åˆ«æœ‰ä¸€ä¸ªæŸ¥è¯¢ (query) ã€é”® (key) ä»¥åŠå€¼ (value)ï¼Œè€Œå¤šæŸ¥è¯¢æ³¨æ„åŠ›æ–¹æ¡ˆæ”¹ä¸ºåœ¨æ‰€æœ‰å¤´ä¸Šå…±äº«åŒä¸€ä¸ªé”®å’Œå€¼ã€‚è¿™ä¸ªæŠ€å·§å¯¹é¢„è®­ç»ƒå½±å“ä¸å¤§ï¼Œä½†å®ƒæå¤§åœ° [æé«˜äº†æ¨ç†çš„å¯æ‰©å±•æ€§](https://arxiv.org/abs/2211.05102): äº‹å®ä¸Šï¼Œ **è¯¥æŠ€å·§å¤§å¤§å‡å°‘äº†è‡ªå›å½’è§£ç æœŸé—´ K,V ç¼“å­˜çš„å†…å­˜å ç”¨ï¼Œå°†å…¶å‡å°‘äº† 10-100 å€** (å…·ä½“æ•°å€¼å–å†³äºæ¨¡å‹æ¶æ„çš„é…ç½®)ï¼Œè¿™å¤§å¤§é™ä½äº†æ¨¡å‹æ¨ç†çš„å†…å­˜å¼€é”€ã€‚è€Œå†…å­˜å¼€é”€çš„å‡å°‘ä¸ºè§£é”æ–°çš„ä¼˜åŒ–å¸¦æ¥äº†å¯èƒ½ï¼Œå¦‚çœä¸‹æ¥çš„å†…å­˜å¯ä»¥ç”¨æ¥å­˜å‚¨å†å²å¯¹è¯ï¼Œä»è€Œä½¿å¾—æœ‰çŠ¶æ€æ¨ç†æˆä¸ºå¯èƒ½ã€‚

- **Date:** 2023-07
- **Pretrain Data Scale:** 2T
- **Language Support:** en,fr
- **Parameter Size:** 1.3B/7.5B/40B/180B



### phi

[![AI Blog](https://img.shields.io/badge/AI%20Blog-Phi%20AI-orange.svg)](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/microsoft/dstoolkit-phi2-finetune) 
[![Hugging Face Space](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/microsoft/phi-2)

**è®­ç»ƒæ•°æ®è´¨é‡ï¼š** åˆ©ç”¨â€œæ•™ç§‘ä¹¦è´¨é‡â€çš„æ•°æ®ï¼Œä¸“æ³¨äºæ—¨åœ¨ä¼ æˆå¸¸è¯†æ¨ç†å’Œå¸¸è¯†çš„åˆæˆæ•°æ®é›†ã€‚åŸ¹è®­è¯­æ–™åº“é€šè¿‡ç²¾å¿ƒæŒ‘é€‰çš„ç½‘ç»œæ•°æ®è¿›è¡Œæ‰©å……ï¼Œå¹¶æ ¹æ®æ•™è‚²ä»·å€¼å’Œå†…å®¹è´¨é‡è¿›è¡Œè¿‡æ»¤ã€‚ï¼ˆPhi-2 leverages â€œtextbook-qualityâ€ data, focusing on synthetic datasets designed to impart common sense reasoning and general knowledge. The training corpus is augmented with carefully selected web data, filtered based on educational value and content quality.ï¼‰

**åˆ›æ–°çš„ç¼©æ”¾æŠ€æœ¯ï¼š** å¾®è½¯åœ¨å¼€å‘ Phi-2 æ—¶ï¼Œé‡‡ç”¨äº†ä»å…¶å‰ä»£æ¨¡å‹ Phi-1.5 åˆ° Phi-2 çš„çŸ¥è¯†æ‰©å±•æŠ€æœ¯ã€‚ä»–ä»¬åˆ©ç”¨äº† Phi-1.5 æ¨¡å‹ä¸­å·²æœ‰çš„çŸ¥è¯†å’Œå­¦ä¹ æˆæœï¼Œå°†è¿™äº›çŸ¥è¯†è½¬ç§»åˆ°æ–°æ¨¡å‹ä¸­ï¼Œä»è€ŒåŠ é€Ÿäº†æ–°æ¨¡å‹è®­ç»ƒçš„æ”¶æ•›é€Ÿåº¦ã€‚ç®€å•æ¥è¯´ï¼Œå°±æ˜¯è®©æ–°æ¨¡å‹åœ¨å­¦ä¹ åˆæœŸå°±èƒ½ç«™åœ¨ä¸€ä¸ªæ›´é«˜çš„èµ·ç‚¹ä¸Šï¼Œå¿«é€Ÿè¾¾åˆ°é«˜æ€§èƒ½ã€‚è¿™ç§çŸ¥è¯†è½¬ç§»çš„æ–¹æ³•ä¸ä»…æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œè¿˜æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­çš„å¾—åˆ†ã€‚

**ä¼˜åŒ–çš„Transformerç»“æ„**ï¼šPhi-2ç ”ç©¶äººå‘˜å¼•å…¥äº†è‡ªå®šä¹‰ä¼˜åŒ–ä»¥æœ€å¤§é™åº¦åœ°æé«˜æ•ˆç‡ã€‚

- **Date:** 2023-12
- **Pretrain Data Scale:** 1.4T
- **Language Support:** en
- **Parameter Size:** 1B/1.5B/2B



### phi3

[![arXiv](https://img.shields.io/badge/arXiv-2404.14219-b31b1b.svg)](https://arxiv.org/abs/2404.14219) 
[![Hugging Face Space](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)

é‡‡ç”¨äº†transformer decoderæ¶æ„ï¼Œå…¶é»˜è®¤çš„ä¸Šä¸‹æ–‡é•¿åº¦ä¸º**4K token**ã€‚å›¢é˜Ÿè¿˜é€šè¿‡LongRopeæŠ€æœ¯æ¨å‡ºäº†é•¿ä¸Šä¸‹æ–‡ç‰ˆæœ¬ï¼Œå°†ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•åˆ°**128K token**ã€‚ç”±äºä½“ç§¯å°å·§ï¼Œphi-3-miniæ¨¡å‹å¯ä»¥è¢«é‡åŒ–ä¸º**4 bit**ï¼Œä½¿å…¶å†…å­˜å ç”¨ä»…çº¦ä¸º**1.8GB**ã€‚å›¢é˜Ÿé€šè¿‡åœ¨é…å¤‡A16ä»¿ç”ŸèŠ¯ç‰‡çš„iPhone 14ä¸Šéƒ¨ç½²phi-3-miniè¿›è¡Œäº†æµ‹è¯•ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å®Œå…¨ç¦»çº¿çš„çŠ¶æ€ä¸‹åŸç”Ÿè¿è¡Œï¼Œå¹¶å®ç°äº†æ¯ç§’å¤„ç†è¶…è¿‡12ä¸ªtokensçš„é€Ÿåº¦ã€‚

è®­ç»ƒæ•°æ®é›†ä¹Ÿæ˜¯åˆ›æ–°ç‚¹ä¹‹ä¸€ï¼ŒåŒ…æ‹¬ç»è¿‡ä¸¥æ ¼è¿‡æ»¤çš„ç½‘ç»œæ•°æ®å’Œåˆæˆæ•°æ®ï¼Œè¿™äº›æ•°æ®ç»è¿‡ç²¾å¿ƒç­›é€‰å’Œä¼˜åŒ–ï¼Œä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿæ˜¾è‘—å‡å°æ¨¡å‹çš„å¤§å°è€Œä¸å½±å“æ€§èƒ½ã€‚

- **Date:** 2024-04
- **Pretrain Data Scale:** 3.3Tï½4.8T
- **Language Support:** en
- **Parameter Size:** 3.8B/7B/14B



### Gemma

- **Date:** 2024-04
- **Pretrain Data Scale:** 3.3Tï½4.8T
- **Language Support:** en
- **Parameter Size:** 2B/7B



## æ¨¡å‹è¯„ä¼°å¹³å°

### OpenLLM Leaderboard by Hugging Face[[Leaderboard]](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)

Hugging Faceçš„OpenLLMæ’è¡Œæ¦œæ˜¯ä¸€ä¸ªæ—¨åœ¨è¿½è¸ªã€è¯„çº§å’Œè¯„ä¼°å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’ŒèŠå¤©æœºå™¨äººçš„å¹³å°ã€‚è¿™ä¸ªæ’è¡Œæ¦œæä¾›äº†ä¸€ä¸ªé›†ä¸­çš„å¹³å°ã€‚

[Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) is actually just a wrapper running the open-source benchmarking library [Eleuther AI LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) created by the [EleutherAI non-profit AI research lab](https://www.eleuther.ai/) famous for creating [The Pile](https://pile.eleuther.ai/) and training [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6b), [GPT-Neo-X 20B](https://huggingface.co/EleutherAI/gpt-neox-20b), and [Pythia](https://github.com/EleutherAI/pythia). A team with serious credentials in the AI space!ï¼ˆCitation from Hugging Face blogï¼‰



### Chatbot Arena by LMSYS and SkyLab[[Chatbot Arena]]([Chat with Open Large Language Models (lmsys.org)](https://arena.lmsys.org/))

Chatbot Arenaçš„ä¸»è¦åŠŸèƒ½ä»‹ç»ï¼š

1. **æ¨¡å‹å¯¹æˆ˜ï¼ˆArena Battleï¼‰**ï¼šåœ¨Chatbot Arenaä¸­ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©ä¸¤ä¸ªä¸åŒçš„åŒ¿åæ¨¡å‹ï¼ˆå¦‚ChatGPTã€Claudeã€Llamaç­‰ï¼‰ï¼Œå¹¶å°†å®ƒä»¬è¿›è¡Œå¯¹æ¯”ã€‚åœ¨ä¸€ä¸ªå®‰å…¨çš„æ¯”èµ›ç¯å¢ƒä¸­ï¼Œç”¨æˆ·å¯ä»¥æé—®å¹¶è¯„ä¼°ä¸¤ä¸ªæ¨¡å‹çš„å›ç­”ï¼Œé€šè¿‡æŠ•ç¥¨å†³å®šå“ªä¸ªæ¨¡å‹è¡¨ç°æ›´å‡ºè‰²ã€‚è¿™ä¸€æ¯”è¾ƒè¿‡ç¨‹å¯ä»¥å¤šè½®è¿›è¡Œï¼Œç›´åˆ°ç¡®å®šæœ€ç»ˆçš„èƒœåˆ©è€…ã€‚ä¸ºäº†ä¿è¯æ¯”èµ›çš„å…¬å¹³æ€§ï¼Œå¦‚æœæŸè½®å¯¹è¯æš´éœ²äº†æ¨¡å‹çš„å…·ä½“èº«ä»½ï¼Œè¯¥è½®çš„æŠ•ç¥¨ç»“æœå°†è¢«æ’é™¤ã€‚
2. **å®æ—¶èŠå¤©ï¼ˆDirect Chatï¼‰**ï¼šChatbot Arenaæä¾›äº†ä¸€ä¸ªå®æ—¶èŠå¤©åŠŸèƒ½ï¼Œå…è®¸ç”¨æˆ·ç›´æ¥ä¸é€‰å®šçš„æ¨¡å‹è¿›è¡Œå¯¹è¯ã€‚æ— è®ºç”¨æˆ·é€‰æ‹©çš„æ˜¯æ–‡æœ¬è¿˜æ˜¯è§†è§‰èŠå¤©æ¨¡å¼ï¼Œéƒ½èƒ½å³æ—¶æ¥æ”¶åˆ°æ¨¡å‹çš„åé¦ˆã€‚
3. **æ’è¡Œæ¦œï¼ˆLeaderboardï¼‰**ï¼šChatbot Arenaé€šè¿‡åˆ†æè¶…è¿‡300,000ä¸ªäººç±»ç”¨æˆ·çš„æŠ•ç¥¨ç»“æœï¼Œé‡‡ç”¨åŸºäºEloç³»ç»Ÿçš„æ–¹æ³•æ¥è¯„ä¼°å„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ’åã€‚è¿™è®©ç”¨æˆ·èƒ½å¤Ÿç›´è§‚åœ°äº†è§£å“ªäº›æ¨¡å‹å½“å‰å¤„äºé¢†å…ˆåœ°ä½ï¼Œè½»æ¾è¯†åˆ«å‡ºLLMé¢†åŸŸçš„ä½¼ä½¼è€…ã€‚



## benchmarks

### ARC (AI2 Reasoning Challenge)

[![arXiv](https://img.shields.io/badge/arXiv-2404.14219-b31b1b.svg)](https://arxiv.org/abs/2404.14219) 
[![Hugging Face Space](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)

This benchmark tests a model's ability to reason and solve grade-school level science questions. It's designed to challenge models to think more deeply about a problem rather than relying on superficial patterns in data.

### HellaSwag

This benchmark aims to assess the model's ability to complete sentences in a sensible manner. It focuses on common sense reasoning and predicting plausible sentence endings in a narrative context.

### MMLU (Massive Multitask Language Understanding)

This is a broad and diverse benchmark that evaluates a model's understanding across a wide range of subjects, from humanities to science. It's designed to test the model's general knowledge and understanding capabilities on various topics.

> æ³¨æ„ï¼šMMLUçš„è¯„ä¼°åŒ…å«ä¸‰ç§å½¢å¼ï¼Œåˆ†åˆ«æ˜¯MMLU (HELM)ã€MMLU (Harness)å’ŒMMLU (Original)ã€‚æ›¾ç»ï¼Œåœ¨Hugging Faceçš„OpenLLMæ’è¡Œæ¦œä¸Šï¼Œ[**LLaMA model ğŸ¦™**](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)è¢«çˆ†å‡ºMMLUè¯„ä¼°ç»“æœæ˜æ˜¾ä½äº[å·²å‘è¡¨çš„ LLaMa è®ºæ–‡](https://arxiv.org/abs/2302.13971)ä¸­çš„æ•°å­—ã€‚æœ€ç»ˆ[Blog](https://huggingface.co/blog/open-llm-leaderboard-mmlu)ä¸Šçš„å£°æ˜å°†è¿™ä¸ªç°è±¡å½’ç»“ä¸ºç”¨äºæµ‹è¯•çš„MMLU benchmarkçš„ç»†èŠ‚ä¸åŒå¯¼è‡´äº†ç»“æœä¸åŒï¼Œè¯¦æƒ…å¯å‚è§[Blog](https://huggingface.co/blog/open-llm-leaderboard-mmlu)

### TruthfulQA

This benchmark evaluates a model's ability to provide truthful answers. It focuses on minimizing the spread of misinformation by testing how models handle questions that might typically lead to untruthful or misleading answers.

### Winogrande

An extension of the Winograd Schema Challenge, this benchmark tests a model's ability to resolve coreference ambiguities in sentences, which is critical for understanding relationships and context within text.

### GSM8K (Grade School Math 8K)

This benchmark evaluates a model's ability to solve mathematical problems typically found at the grade school level. It tests the model's numerical and reasoning abilities in a structured question format.



## Quantitative metrics for large model inference

Quantitative metrics for large model inference using NVIDIA GPUs and Apple Silicon chips, please reach [here](https://github.com/XiongjieDai/GPU-Benchmarks-on-LLM-Inference)

