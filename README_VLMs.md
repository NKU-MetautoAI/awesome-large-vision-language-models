# awesome-list-of-LLM_VLMsğŸŒâœ¨
github awesome list of recent LLMs and VLMs.

Here is list of **VLMs**, to reach list of LLMs. Click [here](https://github.com/HuBocheng/awesome-list-of-LLM_VLMs/blob/master/README.md)ğŸš€



## Quick StartğŸ

æŒ‰å‘å¸ƒæ—¶é—´æ’åº

|   Model    |       Parameters        |                             Demo                             |                          CheckPoint                          | Details |
| :--------: | :---------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :-----: |
| MiniGemini<br />(MGM) |       2B/7B/13B/34B       | [MGM](https://huggingface.co/spaces/jiaqianjing/Mini-Gemini) | [MGM  Family](https://huggingface.co/collections/YanweiLi/mgm-6603c50b9b43d044171d0854) |    [MiniGemini](#minigemini)     |
|   Bunny    |         2B/3B/4B/8B         | [Bunny](http://bunny.dataoptim.org/) | [BAAI](https://huggingface.co/BAAI) |     [Bunny](#bunny)    |
|   Llava    | 7B/13B | [Llava v1.6](https://huggingface.co/spaces/liuhaotian/LLaVA-1.6) | [Llava v1.5](https://huggingface.co/collections/liuhaotian/llava-16-65b9e40155f60fd046a5ccf2)<br />[Lava v1.6](https://huggingface.co/collections/liuhaotian/llava-15-653aac15d994e992e2677a7e) |    [Llava](#llava)     |
| Cog Series |    17B/18B    | [CogVLM & CogAgent](https://huggingface.co/spaces/THUDM/CogVLM-CogAgent) |            [THUDM ](https://huggingface.co/THUDM)            |    [Cog Series](#cog-series)      |
|    HPT     |       3-8B/6B | NONE |            [HPT](https://huggingface.co/HyperGAI)            |    [HPT](#hpt)    |
| MiniGPT4 Series | 7B/13B | Invalid Now | [Vision-CAIR ](https://huggingface.co/Vision-CAIR) | [MiniGPT4 Series](#minigpt4-series) |
| TinyLLaVA | 1.4B/2.4B/3.1B | NONE | [TinyLLaVA](https://huggingface.co/tinyllava) | [TinyLLaVA](#tinyllava) |
| TinyGPT-V |  | | [TinyGPT-V]() | [TinyGPT-V](#tinygpt-v) |
| PaliGemma | 3B | [PaliGemma](https://huggingface.co/spaces/big-vision/paligemma) | [PaliGemma Family](https://huggingface.co/collections/google/paligemma-ft-models-6643b03efb769dad650d2dda) | [PaliGemma](#paligemma) |
| PaLI-3 | 5B |  |  | [PaLI-3](#palI-3) |

## Details Regarding Models AboveğŸ“Š

### MiniGemini

[![arXiv](https://img.shields.io/badge/arXiv-2403.18814-b31b1b.svg)](https://arxiv.org/pdf/2403.18814) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/dvlab-research/MGM)
[![Hugging Face Collections](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Collections-blue)](https://huggingface.co/collections/YanweiLi/mgm-6603c50b9b43d044171d0854)


##### åŠ¨æœº

ä¸ºäº†ç¼©å°å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸å…ˆè¿›æ¨¡å‹ï¼ˆå¦‚GPT-4å’ŒGeminiï¼‰ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œé€šè¿‡æŒ–æ˜VLMsçš„æ½œåŠ›ï¼Œæé«˜å…¶åœ¨è§†è§‰ç†è§£ã€æ¨ç†å’Œç”Ÿæˆæ–¹é¢çš„è¡¨ç°ã€‚è¿™ä¸ªåŠ¨æœºæ¥æºäºå¯¹VLMsåœ¨é«˜åˆ†è¾¨ç‡è§†è§‰æ ‡è®°ã€é«˜è´¨é‡æ•°æ®å’ŒVLMå¼•å¯¼ç”Ÿæˆæ–¹é¢çš„æ½œåŠ›çš„æ¢ç´¢ï¼Œä»¥æœŸæå‡å…¶æ€§èƒ½å’Œæ‹“å±•åº”ç”¨èŒƒå›´ã€‚

##### åˆ›æ–°ç‚¹

ä¸»è¦ä»ä¸‹é¢ä¸‰ä¸ªæ–¹é¢æŒ–æ˜VLMçš„æ½œèƒ½

1. **High-Resolution Visual Tokens**: Initially, ConvNets are used to generate high-resolution images to enhance image detail. To minimize computational resource expenditure, the author further proposes optimizing for high-resolution without increasing the number of visual tokens, by employing an additional visual encoder.
2. **High-Quality Data**: To bolster data quality, the author amalgamates high-quality datasets from diverse public sources, ensuring a rich and varied foundational dataset.
3. **VLM Guided Generation**: By integrating with a text-to-image model, the capability for image generation is enhanced.

Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs) from 2B to 34B. It is demonstrated to achieve leading performance in several zero-shot benchmarks and even surpasses the developed private models



##### Architecture

<div align="center">
  <img src="./image/minigemini1.png" alt="image-20240510165317066" width="600" />
</div>

<div align="center">
  <img src="./image/minigemini2.png" alt="image-20240510165317066" width="600" />
</div>

In conclusion, dual vision encoders are utilized to provide low-resolution visual embedding and high-resolution candidates; patch info mining is proposed to conduct patch-level mining between high-resolution regions and low-resolution visual queries; LLM is utilized to marry text with images for both comprehension and generation at the same time.

The enhancements are further supported by employing an end-to-end workflow, a dual-resolution visual encoder design for high and low resolution, and a patch info mining module. For detailed information, refer to the paper.



### Bunny

[![arXiv](https://img.shields.io/badge/arXiv-2402.11530-b31b1b.svg)](https://arxiv.org/pdf/2402.11530) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/BAAI-DCAI/Bunny)

For more information on the Bunny model checkpoints, please refer to the GitHub link above or click [here](https://github.com/BAAI-DCAI/Bunny). This includes the fully trained checkpoints (for evaluation), the pre-trained checkpoints, and more.

##### åŠ¨æœº

æƒ³è¦beat the scaling lawï¼Œè§£å†³å¤§å‹MLLMçš„è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œé€šè¿‡æ„å»ºæ›´å…·ä¿¡æ¯æ€§çš„è®­ç»ƒæ•°æ®æ¥è®­ç»ƒå‡ºæ€§èƒ½ä¼˜è¶Šçš„è¾ƒå°MLLMï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚



##### åˆ›æ–°ç‚¹

è¯¥å·¥ä½œ focus on data optimization to compensate for the reduction in model sizeï¼Œå¹¶ä¸”é€šè¿‡æ•°æ®é›†å‹ç¼©æ„å»ºäº†ä¿¡æ¯é‡æ›´å¤§çš„è®­ç»ƒæ•°æ®ï¼Œå³ä»æ›´å¹¿æ³›çš„æ¥æºä¸­ç²¾é€‰æ•°æ®ã€‚

1. **é¢„è®­ç»ƒæ•°æ®é›†çš„æ„å»º**ï¼šæ•´ä¸ªè¿‡ç¨‹æ¶‰åŠä¸€ä¸ªç²¾ç»†çš„ä¸‰æ­¥æ ¸å¿ƒé€‰æ‹©æ–¹æ¡ˆï¼ŒåŸºäºCLIPåµŒå…¥

   - **èšç±»å’Œå›¾æ„å»º**ï¼šå—SemDeDupæ–¹æ³•çš„å¯å‘ï¼Œä»–ä»¬é¦–å…ˆä½¿ç”¨k-meansç®—æ³•å¯¹æ‰€æœ‰20äº¿ä¸ªå›¾åƒåµŒå…¥è¿›è¡Œèšç±»ã€‚åœ¨æ¯ä¸ªç°‡ä¸­ï¼Œä»–ä»¬åˆ›å»ºä¸€ä¸ªæ— å‘å›¾ï¼Œå…¶ä¸­çš„èŠ‚ç‚¹ï¼ˆå›¾åƒåµŒå…¥ï¼‰åœ¨ä½™å¼¦ç›¸ä¼¼åº¦è¶…è¿‡é¢„å®šä¹‰é˜ˆå€¼ï¼ˆæœ¬ä¾‹ä¸­ä¸º0.86ï¼‰æ—¶ç›¸è¿ã€‚è¿™ä¸€æ­¥æœ‰åŠ©äºè¯†åˆ«ç›¸ä¼¼çš„å›¾åƒå¹¶å‡å°‘å†—ä½™ã€‚
   - **å­å›¾è¿‡æ»¤**ï¼šå¯¹äºç°‡ä¸­å½¢æˆçš„æ¯ä¸ªè¿é€šå­å›¾ï¼Œåªä¿ç•™ä¸€ä¸ªæ ·æœ¬ â€”â€” å³å…¶åˆ°ç°‡è´¨å¿ƒçš„æ¬§å‡ é‡Œå¾—è·ç¦»å¤„äºä¸­ä½æ•°çš„æ ·æœ¬ã€‚æ­¤æ–¹æ³•æœ‰æ•ˆåœ°å°†æ ·æœ¬é‡å‡å°‘åˆ°9.52äº¿å¼ å›¾åƒï¼ŒåŒæ—¶ç¡®ä¿æ¯ä¸ªç°‡ä¸­æœ€å…·ä»£è¡¨æ€§çš„æ ·æœ¬è¢«ä¿ç•™ã€‚
   - **åŸºäºæ–‡æœ¬-å›¾åƒç›¸ä¼¼åº¦çš„è´¨é‡è¿‡æ»¤**ï¼šç„¶åï¼Œä»–ä»¬é€šè¿‡å¯¹æ ·æœ¬åŸºäºæ¯ä¸ªæ ·æœ¬çš„æ–‡æœ¬åµŒå…¥å’Œå¯¹åº”å›¾åƒåµŒå…¥çš„ä½™å¼¦ç›¸ä¼¼åº¦è¿›è¡Œæ’åºï¼Œç»§ç»­ç²¾ç‚¼è¿™ä¸ªå­é›†ã€‚é€šè¿‡é€‰æ‹©æ’ååœ¨40%åˆ°60%ä¹‹é—´çš„æ ·æœ¬ï¼Œä»–ä»¬æ¶ˆé™¤äº†è¾ƒä½è´¨é‡çš„å›¾åƒ-æ–‡æœ¬å¯¹ï¼Œå°†æ•°æ®é›†è§„æ¨¡è¿›ä¸€æ­¥ç¼©å‡åˆ°1.9äº¿ã€‚
   - **æ•æ‰å¤šæ ·æ€§å’Œæœ¬è´¨**ï¼šå‰©ä½™çš„æ ·æœ¬æŒ‰ç…§æ¯ä¸ªå›¾åƒåµŒå…¥ä¸å…¶ç°‡è´¨å¿ƒçš„ä½™å¼¦ç›¸ä¼¼åº¦è¿›è¡Œæ’åºã€‚è¿™é‡Œï¼Œä»–ä»¬ä¿ç•™æ’ååœ¨15%åˆ°35%ä¹‹é—´çš„æ ·æœ¬ã€‚è¿™ä¸€æ­¥ç¡®ä¿äº†æœ€ç»ˆçš„å­é›†ï¼Œç°åœ¨å‡å°‘åˆ°3800ä¸‡ï¼Œæ•è·äº†åŸå§‹LAION-2Bæ•°æ®é›†çš„æœ¬è´¨å’Œå¤šæ ·æ€§ã€‚
   - **æœ€ç»ˆæŠ½æ ·ä»¥æé«˜è®­ç»ƒæ•ˆç‡**ï¼šä»è¿™ä¸ªç²¾ç‚¼çš„3800ä¸‡æ ¸å¿ƒé›†ä¸­ï¼ŒéšæœºæŠ½å–200ä¸‡ä¸ªæ ·æœ¬ï¼Œå½¢æˆæœ€ç»ˆçš„æ•°æ®é›†ï¼Œå‘½åä¸ºBunny-pretrain-LAION-2Mã€‚é€‰æ‹©è¿™ä¸ªè§„æ¨¡æ˜¯ä¸ºäº†å¹³è¡¡æ•°æ®çš„ä¸°å¯Œæ€§ä¸è®­ç»ƒçš„æˆæœ¬å’Œæ•ˆç‡ã€‚

2. è¯¥å·¥ä½œæ”¶é›†äº†ä¸€ç»„è§†è§‰æŒ‡ä»¤å¾®è°ƒæ•°æ®é›† â€” DataOptim1ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œä»–ä»¬æ¢ç´¢äº†æ›´å¥½çš„å¾®è°ƒæ•°æ®é›†ç»„åˆã€‚å…·ä½“è€Œè¨€ï¼Œä»–ä»¬åˆ©ç”¨äº†SVIT-mix-665K [17] å¹¶åœ¨å…¶ä¸­ç”¨WizardLM-evol-instruct-70K [33] æ›¿æ¢äº†ShareGPT-40K [26]ï¼Œä»è€Œå½¢æˆäº†Bunny-695Kæ•°æ®é›†ã€‚

   >ä»–ä»¬å‘ç°ï¼Œå°†å¤šæ¨¡æ€å­¦ä¹ è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å¤šæ¨¡æ€æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒå¯èƒ½ä¼šæŸå®³å…¶ä»é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»§æ‰¿çš„è®¤çŸ¥èƒ½åŠ›ã€‚è¿™å¯èƒ½æ˜¯ç”±äºå¤šæ¨¡æ€è®­ç»ƒæ•°æ®ä¸­æ–‡æœ¬ä¿¡æ¯è¾ƒå°‘ä¸”å¤šæ ·æ€§è¾ƒä½æ‰€è‡´ã€‚åœ¨å¾®è°ƒæ•°æ®é›†ä¸­ä¿ç•™ä¸€å®šé‡çš„é«˜è´¨é‡çº¯æ–‡æœ¬æ•°æ®å¯ä»¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚

3. ä¸€ä¸ªå³æ’å³ç”¨çš„VLMæ¡†æ¶ï¼Œè®¾è®¡EVA-CLIP å’Œ SigLIPç­‰è§†è§‰ç¼–ç å™¨å’ŒPhi-1.5ã€StableLM-2 å’Œã€Phi-2ç­‰å¤§è¯­è¨€æ¨¡å‹.

##### Architecture

<div align="center">
  <img src="./image/bunny.png"  width="600" />
</div>


### Llava

[![arXiv](https://img.shields.io/badge/arXiv-2304.08485-b31b1b.svg)](https://arxiv.org/abs/2304.08485) 

[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/haotian-liu/LLaVA)

ä½œä¸ºVLMsé¢†åŸŸè¾ƒä¸ºæ—©èµ·çš„å·¥ä½œï¼ŒLlavaç»“åˆäº†é¢„è®­ç»ƒçš„ CLIP ViT-L/14 è§†è§‰ç¼–ç å™¨å’Œ Vicuna è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸”é€šè¿‡ä¸€ä¸ªç®€å•çš„æŠ•å½±çŸ©é˜µè¿æ¥ä¸¤è€…æ¥å®ç°å¤šæ¨¡æ€èƒ½åŠ›ã€‚

åˆ›æ–°ç‚¹

1. Llavaä½¿ç”¨çš„ä¹Ÿæ˜¯ç»å…¸çš„é¢„è®­ç»ƒ+å¾®è°ƒçš„è®­ç»ƒæ­¥éª¤
2. åˆ›æ–°æ€§åœ°æå‡ºäº†**æ–°çš„æ•°æ®é›†ç»„ç»‡æ–¹å¼**ï¼Œå°±æ˜¯åˆ©ç”¨GPT-4 ç”Ÿæˆä¸å›¾åƒç›¸å…³çš„æŒ‡ä»¤é—®é¢˜ï¼Œä¾‹å¦‚ä½¿ç”¨åŒ…å«äº†å¤§é‡çš„å›¾åƒåŠå…¶å¯¹åº”çš„æ–‡æœ¬æè¿°çš„COCOæ•°æ®é›†ï¼Œå°†å›¾æ–‡å¯¹å„¿ä¼ è¾“ç»™GPT-4å¹¶ä¸”ç»„ç»‡æ–°çš„é—®é¢˜ï¼Œ**åˆ©ç”¨AIäº§å‡ºè®­ç»ƒæ•°æ®æ¥è®­ç»ƒAI**ã€‚
>ä¸ºäº†ç¡®ä¿ç”Ÿæˆçš„æ•°æ®å…·æœ‰å¤šæ ·æ€§å’Œæ·±åº¦ï¼Œç ”ç©¶äººå‘˜è®¾è®¡äº†ä¸‰ç§ä¸åŒç±»å‹çš„æŒ‡ä»¤-å“åº”å¯¹ï¼š
>- å¯¹è¯å¼æ•°æ®ï¼ˆConversationï¼‰ï¼š æ¨¡æ‹Ÿäººä¸åŠ©æ‰‹ä¹‹é—´çš„å¯¹è¯ï¼Œæ¶‰åŠå…³äºå›¾åƒå†…å®¹çš„å¤šè½®é—®ç­”ã€‚è¿™ç§ç±»å‹çš„æ•°æ®å¯ä»¥å¸®åŠ©æ¨¡å‹å­¦ä¹ å¦‚ä½•è¿›è¡Œè¿è´¯çš„å¤šè½®å¯¹è¯ã€‚
>- è¯¦ç»†æè¿°ï¼ˆDetailed Descriptionï¼‰ï¼š ç”Ÿæˆè¯¦ç»†çš„å›¾åƒæè¿°ï¼Œå¸®åŠ©æ¨¡å‹ç†è§£å’Œç”Ÿæˆè¯¦ç»†çš„è§†è§‰å†…å®¹æè¿°ã€‚
> - å¤æ‚æ¨ç†ï¼ˆComplex Reasoningï¼‰ï¼š ç”Ÿæˆéœ€è¦å¤æ‚æ¨ç†çš„é—®é¢˜å’Œç­”æ¡ˆï¼Œæ¶‰åŠå¤šæ­¥é€»è¾‘æ¨ç†ã€‚è¿™ç§ç±»å‹çš„æ•°æ®èƒ½å¤Ÿæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
3. æ¶æ„ä¸Šä½¿ç”¨äº†Flash Attention 2 å’Œ LoRAï¼ˆä½ç§©è‡ªé€‚åº”ï¼‰ç­‰æŠ€æœ¯ä¼˜åŒ–ï¼Œæé«˜æ•ˆç‡å¹¶å‡å°‘å†…å­˜èµ„æºçš„ä½¿ç”¨

##### Architecture

<div align="center">
  <img src="./image/llava.png"  width="600" />
</div>

##### Llavaçš„æ”¹è¿›

The LLaVa model was proposed in [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) and improved in [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/pdf/2310.03744) by Haotian Liu, Chunyuan Li, Yuheng Li and Yong Jae Lee.è¿™é‡Œå°±æ˜¯Llava1.5çš„è¯ç”Ÿåœ°

1. **Llava1.5**
   - Llava1.5ä¾æ—§ä½¿ç”¨Vicuna ä½œä¸ºåŸºç¡€è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨äº†**ä¸¤å±‚ MLP** æ›¿ä»£äº†åŸæ¥çš„çº¿æ€§æŠ•å½±ï¼ŒåŒæ—¶è¿˜æ”¯æŒæ›´é«˜åˆ†è¾¨ç‡çš„å›¾åƒï¼ˆ336x336 åƒç´ ï¼‰çš„äº¤äº’ã€‚ä½¿ç”¨çš„æ˜¯åˆ’åˆ†å›¾åƒç½‘æ ¼çš„æ–¹å¼ï¼Œå®ç°äº†é«˜åˆ†è¾¨ç‡è¾“å…¥çš„å¤„ç†ï¼Œè¿™æ ·çš„æ–¹å¼ä¹Ÿå·§å¦™åœ°å‡å°‘äº†â€œå¹»è§‰ç°è±¡â€çš„äº§ç”Ÿã€‚
   - å¼•å…¥äº†é¢å‘å­¦æœ¯ä»»åŠ¡çš„æ•°æ®é›†ï¼Œå¦‚VQAï¼ˆVisual Question Answeringï¼‰ã€OCRï¼ˆOptical Character Recognitionï¼‰å’ŒåŒºåŸŸçº§ç†è§£æ•°æ®
2. **Llava1.6** ğŸ”¥
   - æ”¯æŒæ›´é«˜çš„åƒç´ æ•°ï¼ˆå¦‚ 672x672, 336x1344, 1344x336 åˆ†è¾¨ç‡ï¼‰å›¾ç‰‡çš„äº¤äº’
   - æ·»åŠ äº†è§†è§‰æ¨ç†å’Œ OCRï¼ˆå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼‰èƒ½åŠ›



### Cog Series

#### CogVLM

[![arXiv](https://img.shields.io/badge/arXiv-2311.03079-b31b1b.svg)](https://arxiv.org/abs/2311.03079) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/THUDM/CogVLM)

CogVLM is an innovative open-source visual language model (VLM) designed to bridge the gap between traditional language models and visual data processing. It introduces a "visual expert module" within its architecture to enhance the integration of visual and language features without increasing computational demands. This module is embedded in both the attention and feedforward neural network (FFN) layers of a pre-trained language model, allowing for a deep fusion of visual and linguistic data.

Unlike previous methods that often used a shallow alignment strategy, CogVLM achieves a more profound integration by allowing direct interactions between visual and textual representations within the model's layers. This approach enables the model to maintain high performance on natural language processing tasks while also excelling in tasks that require understanding of visual content

  |          æ¨¡å‹åç§°           | è¾“å…¥åˆ†è¾¨ç‡ |                             ä»‹ç»                             |                      Huggingface model                       |                       SAT model                       |
  | :-------------------------: | :--------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :---------------------------------------------------: |
  |      cogvlm-chat-v1.1       |    490     |      æ”¯æŒåŒæ—¶è¿›è¡Œå¤šè½®èŠå¤©å’Œè§†è§‰é—®ç­”ï¼Œæ”¯æŒè‡ªç”±çš„æç¤ºè¯ã€‚      |     [link](https://huggingface.co/THUDM/cogvlm-chat-hf)      | [link](https://huggingface.co/THUDM/CogVLM/tree/main) |
  |       cogvlm-base-224       |    224     |               æ–‡æœ¬-å›¾åƒé¢„è®­ç»ƒåçš„åŸå§‹æ£€æŸ¥ç‚¹ã€‚                |   [link](https://huggingface.co/THUDM/cogvlm-base-224-hf)    | [link](https://huggingface.co/THUDM/CogVLM/tree/main) |
  |       cogvlm-base-490       |    490     | é€šè¿‡ä» cogvlm-base-224 è¿›è¡Œä½ç½®ç¼–ç æ’å€¼ï¼Œå°†åˆ†è¾¨ç‡æå‡åˆ°490ã€‚ |   [link](https://huggingface.co/THUDM/cogvlm-base-490-hf)    | [link](https://huggingface.co/THUDM/CogVLM/tree/main) |
  | cogvlm-grounding-generalist |    490     |    æ­¤æ£€æŸ¥ç‚¹æ”¯æŒä¸åŒçš„è§†è§‰å®šä½ä»»åŠ¡ï¼Œä¾‹å¦‚RECï¼Œå®šä½å­—å¹•ç­‰ã€‚     | [link](https://huggingface.co/THUDM/cogvlm-grounding-generalist-hf) | [link](https://huggingface.co/THUDM/CogVLM/tree/main) |



##### åŠ¨æœº

CogVLMçš„ä½œè€…è®¤ä¸ºæµ…å±‚å¯¹é½æ–¹æ³•ä¹‹æ‰€ä»¥æ€§èƒ½è¾ƒå·®ï¼Œæ˜¯å› ä¸ºå®ƒä»¬ä¾èµ–äºâ€œå†»ç»“â€çš„è¯­è¨€æ¨¡å‹æƒé‡ï¼Œè¿™äº›æƒé‡ç»è¿‡å†…åœ¨è®­ç»ƒä»¥å¤„ç†æ–‡æœ¬æ ‡è®°ï¼Œå­˜åœ¨ä¸¥é‡çš„ä¸åŒ¹é…ã€‚



##### åˆ›æ–°ç‚¹

1. ä¸ºäº†é˜²æ­¢ç›´æ¥åœ¨æ–°æ•°æ®é›†ä¸Šè®­ç»ƒLLMå¯¼è‡´çš„é—å¿˜é—®é¢˜å’Œå¯¹åŸæ•°æ®é›†çš„ä¸ç†Ÿæ‚‰ï¼Œæƒ³è¦åœ¨ä¿è¯åŸæœ‰NLPæ€§èƒ½çš„å‰æä¸‹å¢åŠ è®¾è§‰ç†è§£èƒ½åŠ›
2. CogVLMåœ¨è¯­è¨€æ¨¡å‹ä¸­å¢åŠ äº†ä¸€ä¸ªå¯è®­ç»ƒçš„è§†è§‰ä¸“å®¶ï¼ˆvisual expertï¼‰ã€‚åœ¨å¤„ç†è§†è§‰å’Œè¯­è¨€ä¿¡æ¯æ—¶ï¼Œè§†è§‰ä¿¡æ¯ä¼šé€šè¿‡ä¸€ä¸ªä¸“é—¨çš„æœºåˆ¶æ¥å¤„ç†ï¼Œè€Œä¸æ˜¯ç®€å•åœ°å°†å›¾åƒç‰¹å¾èå…¥åŸæœ‰çš„æ–‡æœ¬å¤„ç†æµç¨‹ã€‚
   - åœ¨æ¯ä¸€å±‚ä¸­ï¼Œå›¾åƒç‰¹å¾ä½¿ç”¨æ–°çš„QKVï¼ˆQuery-Key-Valueï¼‰çŸ©é˜µå’ŒMLPï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰å±‚ï¼Œç‹¬ç«‹äºæ–‡æœ¬ç‰¹å¾è¿›è¡Œå¤„ç†ã€‚
3. ä½¿ç”¨RoPEï¼ˆRotary Positional Embeddingï¼‰ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿä½ç½®ç¼–ç ï¼ˆåœ¨ä¼ ç»Ÿçš„Transformeræ¨¡å‹ä¸­ï¼Œä½ç½®ç¼–ç é€šå¸¸æ˜¯ä¸è¾“å…¥åºåˆ—çš„è¯åµŒå…¥ç›¸åŠ çš„æ–¹å¼æ¥æä¾›åºåˆ—ä¸­æ¯ä¸ªå…ƒç´ çš„ä½ç½®ä¿¡æ¯ã€‚ï¼‰

CogVLMåœ¨è¯­è¨€æ¨¡å‹ä¸­å¢åŠ äº†ä¸€ä¸ªå¯è®­ç»ƒçš„è§†è§‰ä¸“å®¶ï¼ˆvisual expertï¼‰ã€‚è¿™æ„å‘³ç€åœ¨å¤„ç†è§†è§‰å’Œè¯­è¨€ä¿¡æ¯æ—¶ï¼Œè§†è§‰ä¿¡æ¯ä¼šé€šè¿‡ä¸€ä¸ªä¸“é—¨çš„æœºåˆ¶æ¥å¤„ç†ï¼Œè€Œä¸æ˜¯ç®€å•åœ°å°†å›¾åƒç‰¹å¾èå…¥åŸæœ‰çš„æ–‡æœ¬å¤„ç†æµç¨‹ã€‚



##### Architecture

<div align="center">
  <img src="./image/cogVLM.png" alt="image-20240510165317066" width="600" />
</div>




##### ç»†åˆ†

- **CogVLM-Chat**: æ¨¡å‹æ¥å—è‡ªç„¶è¯­è¨€è¾“å…¥å’Œè¾“å‡ºï¼Œä¸»è¦å¤„ç†çº¯æ–‡æœ¬è¾“å…¥å’Œè¾“å‡ºï¼Œé€‚ç”¨äºå¤šç§VQAå’Œå¤šè½®å¯¹è¯æ•°æ®é›†
- **CogVLM-Grounding**ï¼šä¾§é‡äºå¤„ç†åŒ…å«è¾¹ç•Œæ¡†çš„è¾“å…¥å’Œè¾“å‡ºï¼Œæ”¯æŒè§†è§‰åŸºå‡†ç›¸å…³çš„å¤šç§ä»»åŠ¡



##### some tips

LLMæ˜¯åœ¨Vicuna-7Bçš„åŸºç¡€ä¸Šè®­ç»ƒå¾—æ¥çš„ï¼Œä¿è¯å…¶NLPèƒ½åŠ›çš„å‰æä¸‹åŠ ä¸Šäº†è§†è§‰ç†è§£

ç”¨åˆ°äº†P-Tuningå’ŒLoRAä¸¤ç§é«˜æ•ˆå¾®è°ƒæ–¹æ³•



#### CogAgent

[![arXiv](https://img.shields.io/badge/arXiv-2312.08914-b31b1b.svg)](https://arxiv.org/abs/2312.08914) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/THUDM/CogVLM)

**CogAgent** æ˜¯ä¸€ä¸ªåŸºäºCogVLMæ”¹è¿›çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåœ¨CogVLMçš„åŸºç¡€ä¸Šä¸»è¦ä¾§é‡æå‡äº†GUIç†è§£å’Œå¯¼èˆªèƒ½åŠ›ï¼Œèƒ½å¤Ÿè¯†åˆ«å¾®å°çš„é¡µé¢å…ƒç´ å’Œæ–‡æœ¬ï¼Œåœ¨å¤„ç†å±å¹•æˆªå›¾ç›¸å…³çš„ä»»åŠ¡ä¸Šä¼˜äºåŸºäº LLM çš„æ–¹æ³•ã€‚

> å¯ä»¥å¥½å¥½çœ‹ä¸€ä¸‹githubå’Œè®ºæ–‡ä¸­çš„ç¤ºä¾‹ï¼Œå¯ä»¥å¯¹Agentçš„æ¦‚å¿µæœ‰ä¸€ä¸ªç›´è§‚æ„Ÿå—å’Œäº†è§£ã€‚


  |   æ¨¡å‹åç§°    | è¾“å…¥åˆ†è¾¨ç‡ |                             ä»‹ç»                             |                   Huggingface model                   |                        SAT model                        |
  | :-----------: | :--------: | :----------------------------------------------------------: | :---------------------------------------------------: | :-----------------------------------------------------: |
  | cogagent-chat |    1120    |    CogAgentçš„èŠå¤©ç‰ˆæœ¬ã€‚æ”¯æŒGUIä»£ç†ï¼Œå¤šè½®èŠå¤©å’Œè§†è§‰å®šä½ã€‚     | [link](https://huggingface.co/THUDM/cogagent-chat-hf) | [link](https://huggingface.co/THUDM/CogAgent/tree/main) |
  | cogagent-vqa  |    1120    | CogAgentçš„VQAç‰ˆæœ¬ã€‚åœ¨å•è½®è§†è§‰å¯¹è¯ä¸­å…·æœ‰æ›´å¼ºçš„èƒ½åŠ›ã€‚æ¨èç”¨äºVQAåŸºå‡†æµ‹è¯•ã€‚ | [link](https://huggingface.co/THUDM/cogagent-vqa-hf)  | [link](https://huggingface.co/THUDM/CogAgent/tree/main) |

##### åŠ¨æœº

- å®Œæˆä¸€ä¸ªæ™ºèƒ½ä½“çš„æ„å»ºï¼Œæ”¹æ™ºèƒ½ä½“åœ¨é¢å‘GUIçš„é¢†åŸŸä¸­æœ‰ç€éå¸¸å¥½çš„æ€§èƒ½ã€‚



##### åˆ›æ–°ç‚¹

- **å¯ä»¥å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒ**ï¼šè¯¥å·¥ä½œè®¾è®¡äº†ä¸€ä¸ªäº¤å‰æ³¨æ„åŠ›åˆ†æ”¯ï¼Œå…è®¸åœ¨é€‚å½“çš„è®¡ç®—é¢„ç®—å†…åœ¨åˆ†è¾¨ç‡å’Œéšè—å¤§å°ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œç¼“è§£é«˜åˆ†è¾¨ç‡å›¾åƒéœ€è¦å¤§é‡èµ„æºæ¨ç†å’Œè®¡ç®—çš„é—®é¢˜ã€‚
  - **é«˜åˆ†è¾¨ç‡äº¤å‰æ¨¡å—**å……å½“æ›´é«˜åˆ†è¾¨ç‡è¾“å…¥çš„æ–°åˆ†æ”¯ï¼Œé‡‡ç”¨äº†æ›´å°çš„é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨ï¼Œå¹¶ä½¿ç”¨å°éšè—å°ºå¯¸çš„äº¤å‰æ³¨æ„åŠ›å°†é«˜åˆ†è¾¨ç‡å›¾åƒç‰¹å¾ä¸VLLMè§£ç å™¨çš„æ¯ä¸€å±‚èåˆåœ¨ä¸€èµ·ï¼Œä»è€Œé™ä½è®¡ç®—æˆæœ¬ã€‚
  - **ä¸ä½åˆ†è¾¨ç‡è¾“å…¥åˆ†æ”¯çš„å¯¹æ¯”**ï¼šä¸åŒäºåŸå§‹çš„ä½åˆ†è¾¨ç‡è¾“å…¥åˆ†æ”¯ï¼Œé«˜åˆ†è¾¨ç‡äº¤å‰æ¨¡å—é‡‡ç”¨äº†ä¸€ä¸ªæ›´å°çš„é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨ï¼ˆåœ¨è¿™é‡Œæ˜¯ EVA2-CLIP-L çš„è§†è§‰ç¼–ç å™¨ï¼Œæœ‰ 0.30B å‚æ•°ï¼‰ã€‚è¿™ä¸ªæ¨¡å—ä½¿ç”¨è¾ƒå°çš„éšè—å±‚å°ºå¯¸çš„äº¤å‰æ³¨æ„åŠ›ï¼ˆcross-attentionï¼‰æ¥èåˆé«˜åˆ†è¾¨ç‡å›¾åƒç‰¹å¾ä¸è§†è§‰è¯­è¨€é•¿æœŸè®°å¿†æ¨¡å‹ï¼ˆVLLMï¼‰è§£ç å™¨çš„æ¯ä¸€å±‚ã€‚
  - å¯¹äºä¸€ä¸ªè¾“å…¥å›¾åƒï¼Œæ¨¡å‹ä¼šå°†å…¶é‡æ–°è°ƒæ•´å°ºå¯¸åˆ° 1120 Ã— 1120 å’Œ 224 Ã— 224ï¼Œåˆ†åˆ«é€å…¥é«˜åˆ†è¾¨ç‡äº¤å‰æ¨¡å—å’Œä½åˆ†è¾¨ç‡åˆ†æ”¯ã€‚è¿™ä¸¤ä¸ªåˆ†æ”¯å¹¶è¡Œå·¥ä½œï¼Œå°†å›¾åƒç¼–ç ä¸ºç‰¹å¾åºåˆ— Xhiï¼ˆé«˜åˆ†è¾¨ç‡å›¾åƒç‰¹å¾ï¼‰å’Œ Xloï¼ˆä½åˆ†è¾¨ç‡å›¾åƒç‰¹å¾ï¼‰
- ç§äººæ„å»ºçš„æ•°æ®é›†ï¼šThey notice that the GUI images share a different distribution from natural images. They thus construct a large-scale annotated dataset about GUIs and OCR for continual pre-training.

##### Architecture

<div align="center">
  <img src="./image/cogAgent.png" alt="image-20240510171107123" width="600" />
</div>



### HPT
[![AI Blog](https://img.shields.io/badge/AI%20Blog-hypergai%20AI-orange.svg)](https://hypergai.com/blog/) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/HyperGAI/HPT/) 

æå‡ºçš„Hyper-Pretrained Transformersï¼ˆHPTï¼‰æ¡†æ¶ï¼Œæ˜¯ä¸€ç§å…¨æ–°çš„å¤šæ¨¡æ€LLMé¢„è®­ç»ƒæ¡†æ¶ã€‚å®ƒèƒ½å¤Ÿä»¥é«˜æ•ˆå’Œå¯æ‰©å±•çš„æ–¹å¼è®­ç»ƒå‡ºä¸€ä¸ªå¤§å‹çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹èƒ½å¤Ÿç†è§£å¤šç§æ¨¡æ€çš„è¾“å…¥ã€‚æœ‰HPT Proä¸HPT Airä¸¤ä¸ªç‰ˆæœ¬ã€‚

##### åˆ›æ–°ç‚¹

- åˆ›æ–°ç‰¹æ€§H-Formerä½œä¸ºè§†è§‰ä¸è¯­è¨€æ¨¡æ€ä¹‹é—´çš„æ¡¥æ¢ï¼Œæ˜¯Q-Formerçš„å˜ç§
- H-Former é›†æˆäº†åŒç½‘ç»œè®¾è®¡ï¼Œä»¥å­¦ä¹ å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ï¼Œä»¥å®ç°è§†è§‰è¯­è¨€å¯¹é½ï¼Œä½¿ HPT èƒ½å¤Ÿç†è§£ç»†ç²’åº¦ç»†èŠ‚å’ŒæŠ½è±¡çš„é«˜çº§ä¿¡æ¯

##### Architecture

<div align="center">
  <img src="./image/HPT.png"  width="600" />
</div>





æœ€æ–°çš„VLMs surveyå­˜å‚¨åº“ï¼ŒåŒ…æ‹¬VLMé¢„è®­ç»ƒã€è¿ç§»å­¦ä¹ æ–¹æ³•å’ŒçŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œè¿˜æœ‰å¯ä»¥ä½¿ç”¨çš„æ•°æ®é›†æ±‡æ€»ï¼š[github](https://github.com/jingyi0000/VLM_survey)



### MiniGPT4 Series

#### MiniGPT4



#### MiniGPT4-V2





### TinyLLaVA

[![arXiv](https://img.shields.io/badge/arXiv-2402.14289-b31b1b.svg)](https://arxiv.org/abs/2402.14289) 
[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/DLCV-BUAA/TinyLLaVABench)
[![Hugging Face model](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-model-blue)](https://huggingface.co/tinyllava)







### TinyGPT-V  



### PaLI-3

[[2310.09199\] PaLI-3 Vision Language Models: Smaller, Faster, Stronger (arxiv.org)](https://arxiv.org/abs/2310.09199)

[google-research/big_vision: Official codebase used to develop Vision Transformer, SigLIP, MLP-Mixer, LiT and more. (github.com)](https://github.com/google-research/big_vision?tab=readme-ov-file)

PaLI-3ï¼Œè¿™æ˜¯ PaLI ç³»åˆ—çš„ç¬¬ä¸‰ä»£æ¨¡å‹ã€‚é€šè¿‡ä¸€ä¸ªä»…æœ‰ 5B å‚æ•°çš„é¢„è®­ç»ƒåŸºçº¿æ¨¡å‹ï¼Œä»–ä»¬ä¼˜åŒ–äº†è®­ç»ƒæ–¹æ³•ï¼Œå¹¶åœ¨å¤šä¸ª VLM åŸºå‡†ä¸Šå®ç°äº†æœ‰ç«äº‰åŠ›ä»¥åŠæ–°çš„ SOTA ç»“æœã€‚

æ–°çš„æ–¹æ³•ä¸»è¦ç”±ä¸‰ä¸ªæ­¥éª¤ï¼š

1. **å•æ¨¡æ€é¢„è®­ç»ƒ**ï¼šé¢„è®­ç»ƒå›¾åƒç¼–ç å™¨å’Œæ–‡æœ¬ç¼–ç å™¨ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿå•ç‹¬å¤„ç†å„è‡ªçš„è¾“å…¥ã€‚

   - **å›¾åƒç¼–ç å™¨é¢„è®­ç»ƒ**ï¼šä½¿ç”¨ç½‘é¡µè§„æ¨¡çš„å›¾åƒ-æ–‡æœ¬å¯¹æ•°æ®é›†ï¼ˆå¦‚ WebLIï¼‰ã€‚é‡‡ç”¨ SigLIP è®­ç»ƒæ–¹æ³•ï¼Œå¯¹å›¾åƒå’Œæ–‡æœ¬è¿›è¡Œå¯¹æ¯”é¢„è®­ç»ƒ

     >1. **åµŒå…¥**ï¼šå°†å›¾åƒå’Œæ–‡æœ¬åµŒå…¥åˆ°åŒä¸€é«˜ç»´å‘é‡ç©ºé—´ä¸­ã€‚
     >2. **å¯¹æ¯”æŸå¤±**ï¼šä½¿ç”¨ Sigmoid Cross-Entropy æŸå¤±å‡½æ•°ï¼Œä½¿æ­£ç¡®çš„å›¾åƒ-æ–‡æœ¬å¯¹çš„ç‚¹ç§¯è¾ƒé«˜ï¼Œè€Œä¸æ­£ç¡®çš„å¯¹è¾ƒä½ã€‚

   - **æ–‡æœ¬ç¼–ç å™¨é¢„è®­ç»ƒ**ï¼šä½¿ç”¨ UL2 æ¨¡å‹ï¼Œé‡‡ç”¨â€œå»å™ªå™¨æ··åˆâ€ï¼ˆmixture of denoisersï¼‰çš„æ–¹æ³•è¿›è¡Œè®­ç»ƒã€‚

2. **å¤šæ¨¡æ€è®­ç»ƒ**

   - å°†é¢„è®­ç»ƒçš„å›¾åƒç¼–ç å™¨ï¼ˆViT-G/142ï¼‰ä¸ä¸€ä¸ª 3B å‚æ•°çš„ UL2 ç¼–ç å™¨-è§£ç å™¨è¯­è¨€æ¨¡å‹ç»“åˆã€‚å›¾åƒç¼–ç å™¨å°†å›¾åƒè½¬æ¢ä¸ºè§†è§‰ä»¤ç‰Œï¼Œè¿™äº›ä»¤ç‰Œä¸æ–‡æœ¬ä»¤ç‰Œä¸€èµ·è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­ã€‚
   - åœ¨è¿™ä¸ªé˜¶æ®µï¼Œ**å›¾åƒç¼–ç å™¨ä¿æŒå†»ç»“çŠ¶æ€ï¼Œåªè®­ç»ƒè¯­è¨€æ¨¡å‹éƒ¨åˆ†**ï¼Œä»¥ç¡®ä¿å›¾åƒç¼–ç å™¨çš„åµŒå…¥è´¨é‡ã€‚
   - **é«˜åˆ†è¾¨ç‡è®­ç»ƒ**:å¼€å§‹æ—¶ä½¿ç”¨è¾ƒä½åˆ†è¾¨ç‡çš„å›¾åƒï¼Œé€æ­¥æé«˜åˆ†è¾¨ç‡ï¼Œä»¥æé«˜æ¨¡å‹å¯¹å›¾åƒç»†èŠ‚çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚ä¸­é—´ä¼šåœ¨ 812Ã—812 å’Œ 1064Ã—1064 åˆ†è¾¨ç‡ä¸Šä¿å­˜æ£€æŸ¥ç‚¹ã€‚

3. **åˆ†è¾¨ç‡å¢åŠ å’Œä»»åŠ¡ä¸“ç”¨å¾®è°ƒ**:

   - è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„è¾“å…¥åˆ†è¾¨ç‡ï¼Œè¿›è¡ŒçŸ­æœŸå¾®è°ƒï¼Œè§£å†»å›¾åƒç¼–ç å™¨ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿåœ¨é«˜åˆ†è¾¨ç‡ä¸‹å¤„ç†å›¾åƒã€‚
   - **ä»»åŠ¡ä¸“ç”¨å¾®è°ƒ**ï¼šåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé€šå¸¸ä½¿ç”¨ 812Ã—812 åˆ†è¾¨ç‡æ£€æŸ¥ç‚¹ï¼Œä½†å¯¹äºæŸäº›éœ€è¦æ›´é«˜åˆ†è¾¨ç‡çš„ä»»åŠ¡ï¼ˆå¦‚æ–‡æ¡£ç†è§£ä»»åŠ¡ï¼‰ï¼Œä¼šä½¿ç”¨ 1064Ã—1064 åˆ†è¾¨ç‡è¿›è¡Œå¾®è°ƒã€‚

##### Architecture

<div align="center">
  <img src="./image/paligemma.png"  width="600" />
</div>





### PaliGemma

[PaliGemma  | Google for Developers](https://ai.google.dev/gemma/docs/paligemma?hl=zh-cn)

[PaliGemma â€“ Google's Cutting-Edge Open Vision Language Model (huggingface.co)](https://huggingface.co/blog/paligemma)

[PaliGemma](https://github.com/google-research/big_vision)

PaliGemma ç”± [Transformer è§£ç å™¨](https://arxiv.org/abs/1706.03762)å’Œ [Vision Transformer å›¾åƒç¼–ç å™¨](https://arxiv.org/abs/2010.11929)ç»„æˆï¼Œå…±è®¡æœ‰ 30 äº¿ä¸ªå‚æ•°ã€‚æ–‡æœ¬è§£ç å™¨ä» [Gemma-2B](https://www.kaggle.com/models/google/gemma) åˆå§‹åŒ–ã€‚å›¾ç‰‡ç¼–ç å™¨ä½¿ç”¨ [SigLIP-So400m/14](https://colab.research.google.com/github/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/SigLIP_demo.ipynb?hl=zh-cn) è¿›è¡Œåˆå§‹åŒ–ã€‚PaliGemma æ˜¯æŒ‰ç…§ PaLI-3 æ–¹æ³•è®­ç»ƒçš„ã€‚

PaliGemma å…±å¼€æºäº†ä¸‰ç±»æ¨¡å‹ï¼š
ï¼ˆ1ï¼‰é¢„è®­ç»ƒçš„å¤šæ¨¡æ€åŸºåº§æ¨¡å‹ï¼ˆæ¨¡å‹åœ¨huggingfaceä¸Šæ ‡æ³¨ä¸ºptï¼‰ï¼›
ï¼ˆ2ï¼‰åœ¨å•ä¸ªä»»åŠ¡ä¸Šï¼ˆå¦‚ï¼šDocVQAï¼ŒAI2D ç­‰ï¼‰Finetune å¾—åˆ°çš„æ¨¡å‹ï¼ˆæ¨¡å‹åœ¨huggingfaceä¸Šæ ‡æ³¨ä¸ºftï¼‰ï¼›
ï¼ˆ3ï¼‰æ··åˆæ•°æ®é›†ä¸Š Finetune çš„æ¨¡å‹ï¼ˆæ¨¡å‹åœ¨huggingfaceä¸Šæ ‡æ³¨ä¸ºmixï¼‰ã€‚

è¿™äº›æ¨¡å‹æœ‰ä¸‰ç§ä¸åŒçš„åˆ†è¾¨ç‡ï¼ˆ224x224ã€448x448ï¼‰ã€896x896å’Œä¸‰ç§ä¸åŒçš„ç²¾åº¦ ï¼ˆbf16ã€f16å’Œf32ï¼‰ã€‚æ¯ä¸ªå­˜å‚¨åº“éƒ½åŒ…å«ç»™å®šåˆ†è¾¨ç‡å’Œä»»åŠ¡çš„æ£€æŸ¥ç‚¹ï¼Œæ¯ä¸ªå¯ç”¨ç²¾åº¦éƒ½æœ‰ä¸‰ä¸ªä¿®è®¢ç‰ˆã€‚æ¯ä¸ªå­˜å‚¨åº“çš„mainåˆ†æ”¯éƒ½åŒ…å«float32æ£€æŸ¥ç‚¹ï¼Œå…¶ä¸­bf16as å’Œf16revisions åŒ…å«ç›¸åº”çš„ç²¾åº¦ã€‚å¯¹äºä¸ Transformer å’ŒåŸå§‹ JAX å®ç°å…¼å®¹çš„æ¨¡å‹ï¼Œæœ‰å•ç‹¬çš„å­˜å‚¨åº“ï¼Œé«˜åˆ†è¾¨ç‡æ¨¡å‹éœ€è¦æ›´å¤§çš„å†…å­˜æ¥è¿è¡Œï¼Œå› ä¸ºè¾“å…¥åºåˆ—è¦é•¿å¾—å¤šã€‚å®ƒä»¬å¯èƒ½æœ‰åŠ©äºå¤„ç†é¢—ç²’åº¦è¾ƒä¸ºç²¾ç»†çš„ä»»åŠ¡ï¼ˆå¦‚ OCRï¼‰ï¼Œä½†å¯¹äºå¤§å¤šæ•°ä»»åŠ¡æ¥è¯´ï¼Œè´¨é‡æå‡å¾ˆå°ã€‚

##### Architecture

same as [PaLI-3](#palI-3)

<div align="center">
  <img src="./image/paligemma.png"  width="600" />
</div>




## Response time

The GitHub project [fastestAI]([fastai/fastai: The fastai deep learning library (github.com)](https://github.com/fastai/fastai)) has compiled response time data for recently released LLMs (Large Language Models) and VLMs (Vision Language Models). For detailed statistics, please refer to the table available at this [link](https://thefastest.ai).
